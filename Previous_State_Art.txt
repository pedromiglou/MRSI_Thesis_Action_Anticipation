\section{Data Sources and Sensors}

The first step to anticipating the following action is to know what kind of data we should collect with the sensors. In figure \ref{fig1}, we can see a diagram containing multiple data sources that can be used for human-robot collaboration, along with its advantages and disadvantages.

\begin{figure}[H]
\centering
\includegraphics[width=3.5in]{figs/interaction.PNG}
\caption{Advantages and Disadvantages of several data sources in Human-Robot Collaboration \cite{Mukherjee2022}}
\label{fig1}
\end{figure}

\subsubsection{Gestures}

Humans usually anticipate each other using gestures; therefore, gestures are one of the most common data used to perform action anticipation. Furthermore, it has the advantage of resisting ambient noise.

When gestures are used in action anticipation, they are usually vision based, so there is no need for unnatural movements. With vision, it is possible to include markers such as in \cite{Maeda2016}, but these may lead to occlusions and hinder the worker's movements. Consequently, most work uses markerless vision to allow more unrestricted movements.

Regarding sensors, most of the literature suggests using an RGB camera. Still, some works, such as the one described in \cite{Moutinho2023}, indicate the use of an RGB-D camera to capture both the color and the depth images.

In \cite{Gammulle2019}, \cite{Wu2021}, \cite{Rodriguez2019}, and \cite{Furnari2021}, the authors used, in addition to the color images, the optical flow obtained from them.

\subsubsection{Natural Language}

In addition to gestures, humans also anticipate each other using natural language. The advances in natural language processing make this a possible solution for action anticipation. However, despite being intuitive, simple, effective, and even robust against lighting variations, when it comes to an industrial setting that contains significant sound noise, it becomes less valuable than the alternatives.

\subsubsection{Gaze}

Next, the gaze can also be used to determine where the user's attention resides, which gives a considerable amount of information that can be used to ascertain his following action.

There are two options to obtain the user's gaze. Wearable sensors can provide better results but are expensive and intrusive such as in \cite{Schydlo2018}. On the other hand, algorithms that detect head pose and assume the gaze from it can also be used, which is a cheaper and non-intrusive solution, such as in \cite{Canuto2021}.

\subsubsection{Emotions through Facial Expressions}

Although this is a relatively new idea, some applications analyze the user's emotions from his facial expressions to have even more information in the algorithms.

\subsubsection{Semantics}

Finally, semantic information about the objects can also help the global workflow. For example, suppose the robot is trained to correctly classify which objects are present in the environment, such as in \cite{Furnari2021}. In that case, the temporal information between events can include which objects are in the environment throughout time.

{\color{red}(rephrase this)
Human actions can be recognized semantically through the detection and recognition of human poses or poselets, which are parts of human pose consisting of configurations of limbs. Pose estimation is a fundamental stepping stone to gesture and action recognition or prediction. During action recognition, semantic understanding allows application of a priori knowledge by correlating the action with other features such as pose of body parts, associated objects, and scene segmentation. Thus, each action can be decomposed and built up from the semantics of its features.

Human pose estimation is an essential component for collision avoidance during collaboration, which is driven by semantics
}



% maybe falar aqui do openpose?

\section{Definition and Order of Actions}

After knowing which data must be captured and provided to the algorithm, two problems must be solved. Firstly, it is necessary to specify what the algorithm should consider as an action. Although to humans it may be evident a computer only sees data. Secondly, considering our understanding of action how should the algorithm react to different possible assembly orders.

Regarding the first problem, in the literature, it is subtended that the last frames captured by the camera are considered an action, given that those are the frames that contain the last action made by the user. Although most articles assume this fact and do not go into greater detail, in \cite{Wang2021}, the authors tested how many frames should be considered as the last action to obtain the best results. These tests are evaluated using a metric from {\color{red} (put the reference here)} named per-frame calibrated average precision (cAP).

Regarding the second problem, it is also a topic that is not commonly referred to since most work assumes as a limitation that the assembly order is always the same. However, there are some solutions.

In \cite{Maeda2016}, the authors used a look-up table containing different orders for assembly actions. Then, with the nearest neighbor algorithm, the actions of the human are matched with a specific order. The limitation of this method is that all possible sequences need to be on the table because if they are not there, then the robot will match with a different order which may be undesirable.

In \cite{Zhang2022}, the authors made it so there would be a phase when the robot learned the assembly actions and their order from a demonstration.

%%% shouldn't this be algorithms? %%%%%%%%%%%%%%%%%%
In \cite{Canuto2021}, the authors use an adaptive threshold on the uncertainty of the recurrent neural network, which makes the model need a certain level of certainty to classify the action as a certain class.

In \cite{Schydlo2018}, the authors predicted multiple possible actions while the model was unsure of the following action.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Algorithms}

Machine Learning algorithms have been increasingly more common in the context of action anticipation in collaborative environments. These algorithms can automatically learn from data and make predictions or decisions. The most common strategies in this field are Supervised Learning, Unsupervised Learning, and Reinforcement Learning, whose utility will be analyzed in greater detail in the following subsections.

\subsection{Supervised Learning}

In Supervised Learning, the models are trained using a dataset of labeled data. The models from this group are further divided into Classification, where the new instance is assigned a particular class, and Regression, where it is given a certain real number. These models must generalize the knowledge from the examples to deal with a new instance correctly it has never seen before. Among these models, convolutional and recurrent neural networks are at the forefront of the algorithms to explore.

A Recurrent Neural Network (RNN) is a type of neural network where the output of each time step is fed back into the input at the next time step, allowing the network to remember and incorporate information from previous time steps into its processing of current and future data. This makes RNNs particularly well-suited to processing sequential data, such as text, speech, or time series data which require context or temporal dependencies. In particular, LSTM is an RNN with the ability to keep a memory or state, which makes it better to remember features across several time steps {\color{red}(check this better)}.

A Convolutional Neural Network (CNN) is a type of neural network made up of several convolutional layers which apply a sliding filter over the input reducing its dimension and obtaining its features. Typically, these layers are followed by one or more fully connected layers which perform the prediction using the mentioned features. This makes CNNs an excellent choice to deal with data in a matrix structure such as an image because this input is too massive for manual feature engineering.

The aim of this thesis can be represented as a Classification problem since it is possible to use a sequence of images that must be classified as a particular class or future action. The previous work with this kind of algorithm mainly includes convolutional and recurrent neural networks, with the latter being the most common. 

% LSTM only examples
In \cite{Canuto2021}, the authors employed a Long Short-Term Memory (LSTM) neural network, one of the most common RNNs. In their work, the human skeleton joints were extracted from the camera images over time using OpenPose {\color{red} (put the reference here)}. Then the sequences of 3D points representing the skeleton joints were given to the LSTM as input to perform the desired classification.

In \cite{Furnari2021}, the authors use a Rolling-Unrolling LSTM. The Rolling LSTM (R-LSTM) is a network that continuously encodes the received observations and keeps an updated summary of the past. When it is time to make predictions about future actions, the Unrolling LSTM (U-LSTM) is used with its hidden and cell states equal to the current ones of the R-LSTM. The sequences provided to the LSTM contain RGB frames, optical flow fields, and object-based features.

In \cite{Schydlo2018}, the authors used an encoder-decoder recurrent neural network topology to predict human actions and intent where the encoder and the decoder are both LSTM cells. At each step, the decoder returns a discrete distribution of the possible actions making this algorithm able to consider multiple action sequences.

% LSTM + CNN
Although the previous articles only included RNNs, the most common approach is to use CNNs and RNNs together, such as in \cite{Moutinho2023} and \cite{Zhang2022}. Here the authors take advantage of the CNN to extract features from input images which are then used as the input of an LSTM to obtain the context across the various images.

% Resnet + LSTM
In \cite{Gammulle2019}, the authors used the same idea. Still, instead of training the convolutional part of the network, they used 2 ResNet50's {\color{red} (put the reference here)} which are pre-trained networks to obtain the input features from the image and the optical flow, and 2 LSTM's to take into account both sequences of inputs. Then the two results are merged into a final classification.

% double % might need rework
%% VGG-16 + TTM
In \cite{Wang2021}, the authors used three pre-trained networks: VGG-16, TS, and ConvNet to extract features from the images. Then these features were aggregated using a Temporal Transformer module (TTM), and finally, a progressive prediction module (PPM) will anticipate the worker's future action.

%% CNN
In \cite{Rodriguez2019}, the authors processed the images to obtain motion images and then used a convolutional autoencoder network to generate the following motion images. These images are then passed to a Convolutional Neural Network (CNN) that processes them and makes action predictions for the future. The final action prediction is obtained from the results of the previous network and those of a second CNN, which analyzes the static images.

%% architecture with TSN (very complex overall)
In \cite{Wu2021}, the authors used Temporal Segment Networks (TSN), CNNs, and LSTMs to predict the future frame features and then used them to perform the required classification.

% Look-up table
Apart from deep learning, there are also more classical approaches such as \cite{Maeda2016}, where the authors predicted the following human action using a look-up table containing different orders for assembly actions. With the nearest neighbor algorithm, the actions of the human would be matched with a particular order.

\subsection{Unsupervised Learning}

In Unsupervised Learning, the datasets involved have no labels; therefore, the algorithms aim to find patterns and relationships in the data. This makes them valuable for finding structure in the data, creating clusters based on common characteristics, or identifying anomalies and outliers.

\subsection{Reinforcement Learning}

{\color{red} (refer Learning from Demonstration (LfD))}

In Reinforcement Learning, the model is trained to decide which action to take in a specific environment to maximize a particular reward function. These algorithms learn through trial and error using the reward they obtain in each iteration to improve their performance continuously. This type of learning has a certain resemblance to how humans gain knowledge, and it is useful when there is a need for an agent to make decisions in an environment that has considerable complexity, such as controlling a robot or playing a game.

% POMDP
In \cite{Gorur2018}, the authors used the algorithm Partially Observable Markov Decision Process (POMDP) to handle unexpected conditions, such as when the human's current intention is unknown or irrelevant to the robot or when even though the human's intent is relevant, the human does not need the robot's assistance.

\section{Human-Robot Collaboration Safety}

Finally, safety is a topic that must always be mentioned when robots work with humans, especially in human-robot collaboration. Although collaborative robots or cobots nowadays are made so that if they are interrupted in their work, they switch to a safety mode where they stop, they are still machines with significant strength and can potentially harm the user.

In \cite{Zhang2022}, the authors defined speed limits on the robot and ensured that the robot would avoid the workspace of the human. Then when it needs to move closer to the user, its speed is reduced to guarantee the user's safety.

In \cite{Psarakis2022}, the authors attempted to create a sense of anticipation in humans towards the robot's movements through visual cues of the robot's upcoming action, which is the reverse of what it is being tried to achieve in this dissertation. As with the previous article, they also implemented that the robot must reduce its movement speed when close to the robot. Although it was only tested in Virtual Reality, where the users feel safer, they concluded that the efficiency of the collaboration was increased, and the user had a greater feeling of safety and trust. Furthermore, knowing what the robot will do next also decreases the risk of a collision since the user will avoid the space where the robot is working, increasing safety.

In \cite{Wu2023}, the authors used deep deterministic policy gradient (DDPG) to plan the robot's trajectory so that the robot would not collide with the human to guarantee his safety.

In \cite{Yu2022}, the authors used Q-learning so the robot could automatically generate its motion plans for new tasks by learning from a few kinesthetic human demonstrations. This made the robot adapt itself to different manufacturing environments and efficiently work with non-robotic experts.