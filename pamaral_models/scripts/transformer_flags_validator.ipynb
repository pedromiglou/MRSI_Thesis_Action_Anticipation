{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 00:34:47.189774: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-04 00:34:48.389382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/flags/flags.csv\", header=None)\n",
    "\n",
    "flags = set([(df[1][i],df[2][i],df[3][i]) for i in range(0,14)])\n",
    "\n",
    "colors = [\"red\",\"lightblue\",\"blue\",\"green\",\"orange\",\"yellow\",\"white\"]\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for c1 in colors:\n",
    "    x.append(c1)\n",
    "    if c1 in [x[0] for x in flags]:\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)\n",
    "    \n",
    "    for c2 in colors:\n",
    "        x.append(c1+\",\"+c2)\n",
    "        if c2 in [x[1] for x in flags if x[0]==c1]:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "        \n",
    "        for c3 in colors:\n",
    "            x.append(c1+\",\"+c2+\",\"+c3)\n",
    "            if c3 in [x[2] for x in flags if x[0]==c1 and x[1]==c2]:\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)\n",
    "\n",
    "x = np.array(x)\n",
    "\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " ...\n",
      " [7 7 5]\n",
      " [7 7 6]\n",
      " [7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20, split=\",\")\n",
    "\n",
    "# updates internal vocabulary based on the lyrics\n",
    "tokenizer.fit_on_texts(x)\n",
    "\n",
    "# transform each text in x to a sequence of tokens\n",
    "x = tokenizer.texts_to_sequences(x)\n",
    "\n",
    "x = np.array(pad_sequences(x, maxlen = 3))\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "num = len(y)\n",
    "\n",
    "idx = np.random.permutation(num)\n",
    "\n",
    "x = x[idx]\n",
    "y = y[idx]\n",
    "\n",
    "x = x.reshape((x.shape[0], x.shape[1], 1))\n",
    "\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "np.unique(y)\n",
    "\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 00:34:52.125821: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 00:34:52.196993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 00:34:52.197933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 00:34:52.199568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 00:34:52.200389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 00:34:52.200810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 00:34:53.270044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 00:34:53.270305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 00:34:53.270513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 00:34:53.270680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3347 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 3, 1)        2           ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 3, 1)        7169        ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 3, 1)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 3, 1)        0           ['dropout[0][0]',                \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 3, 1)        2           ['tf.__operators__.add[0][0]']   \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 3, 4)         8           ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 3, 4)         0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 3, 1)         5           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 3, 1)        0           ['conv1d_1[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 3, 1)        2           ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 3, 1)        7169        ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 3, 1)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 3, 1)        0           ['dropout_2[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 3, 1)        2           ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 3, 4)         8           ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 3, 4)         0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 3, 1)         5           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 3, 1)        0           ['conv1d_3[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 3, 1)        2           ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 3, 1)        7169        ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 3, 1)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 3, 1)        0           ['dropout_4[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 3, 1)        2           ['tf.__operators__.add_4[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 3, 4)         8           ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 3, 4)         0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 3, 1)         5           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 3, 1)        0           ['conv1d_5[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 3, 1)        2           ['tf.__operators__.add_5[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 3, 1)        7169        ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 3, 1)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 3, 1)        0           ['dropout_6[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 3, 1)        2           ['tf.__operators__.add_6[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 3, 4)         8           ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 3, 4)         0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 3, 1)         5           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 3, 1)        0           ['conv1d_7[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 3)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          512         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,385\n",
      "Trainable params: 29,385\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 00:35:04.493870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-04 00:35:05.416147: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x1e32b6f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-04 00:35:05.416178: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 960M, Compute Capability 5.0\n",
      "2023-05-04 00:35:05.422268: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-04 00:35:05.658960: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 24s 115ms/step - loss: 0.3234 - accuracy: 0.8697 - val_loss: 0.2460 - val_accuracy: 0.9248\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 39ms/step - loss: 0.2605 - accuracy: 0.9248 - val_loss: 0.2776 - val_accuracy: 0.9248\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 34ms/step - loss: 0.2540 - accuracy: 0.9248 - val_loss: 0.2555 - val_accuracy: 0.9248\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 39ms/step - loss: 0.2581 - accuracy: 0.9248 - val_loss: 0.2432 - val_accuracy: 0.9248\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 37ms/step - loss: 0.2457 - accuracy: 0.9248 - val_loss: 0.2144 - val_accuracy: 0.9248\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 34ms/step - loss: 0.2427 - accuracy: 0.9323 - val_loss: 0.2305 - val_accuracy: 0.9248\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 37ms/step - loss: 0.2219 - accuracy: 0.9223 - val_loss: 0.2072 - val_accuracy: 0.9248\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 38ms/step - loss: 0.2094 - accuracy: 0.9248 - val_loss: 0.1995 - val_accuracy: 0.9298\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 39ms/step - loss: 0.2022 - accuracy: 0.9248 - val_loss: 0.1931 - val_accuracy: 0.9248\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 37ms/step - loss: 0.2016 - accuracy: 0.9298 - val_loss: 0.2035 - val_accuracy: 0.9373\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.2139 - accuracy: 0.9323 - val_loss: 0.2140 - val_accuracy: 0.9248\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 37ms/step - loss: 0.2045 - accuracy: 0.9248 - val_loss: 0.1942 - val_accuracy: 0.9248\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 1s 46ms/step - loss: 0.2081 - accuracy: 0.9248 - val_loss: 0.2054 - val_accuracy: 0.9248\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 1s 41ms/step - loss: 0.2021 - accuracy: 0.9248 - val_loss: 0.1980 - val_accuracy: 0.9248\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 1s 40ms/step - loss: 0.2050 - accuracy: 0.9273 - val_loss: 0.1996 - val_accuracy: 0.9348\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 1s 42ms/step - loss: 0.2160 - accuracy: 0.9348 - val_loss: 0.2095 - val_accuracy: 0.9398\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 1s 46ms/step - loss: 0.2052 - accuracy: 0.9248 - val_loss: 0.1871 - val_accuracy: 0.9248\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 1s 41ms/step - loss: 0.2210 - accuracy: 0.9223 - val_loss: 0.2153 - val_accuracy: 0.9323\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 1s 46ms/step - loss: 0.2090 - accuracy: 0.9273 - val_loss: 0.1870 - val_accuracy: 0.9323\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 1s 41ms/step - loss: 0.1861 - accuracy: 0.9298 - val_loss: 0.1809 - val_accuracy: 0.9323\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 1s 45ms/step - loss: 0.1831 - accuracy: 0.9348 - val_loss: 0.1762 - val_accuracy: 0.9348\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 1s 44ms/step - loss: 0.1825 - accuracy: 0.9298 - val_loss: 0.1717 - val_accuracy: 0.9373\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 1s 41ms/step - loss: 0.1757 - accuracy: 0.9373 - val_loss: 0.1691 - val_accuracy: 0.9373\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 39ms/step - loss: 0.1837 - accuracy: 0.9323 - val_loss: 0.1678 - val_accuracy: 0.9449\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 40ms/step - loss: 0.1784 - accuracy: 0.9348 - val_loss: 0.1675 - val_accuracy: 0.9398\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 1s 40ms/step - loss: 0.1695 - accuracy: 0.9373 - val_loss: 0.1646 - val_accuracy: 0.9449\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 1s 41ms/step - loss: 0.1890 - accuracy: 0.9348 - val_loss: 0.1713 - val_accuracy: 0.9373\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 1s 42ms/step - loss: 0.1729 - accuracy: 0.9373 - val_loss: 0.1843 - val_accuracy: 0.9524\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 1s 41ms/step - loss: 0.1873 - accuracy: 0.9348 - val_loss: 0.1619 - val_accuracy: 0.9398\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 39ms/step - loss: 0.1789 - accuracy: 0.9449 - val_loss: 0.1822 - val_accuracy: 0.9298\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 38ms/step - loss: 0.1787 - accuracy: 0.9348 - val_loss: 0.1718 - val_accuracy: 0.9248\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 36ms/step - loss: 0.1738 - accuracy: 0.9348 - val_loss: 0.1689 - val_accuracy: 0.9424\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 36ms/step - loss: 0.1780 - accuracy: 0.9323 - val_loss: 0.1651 - val_accuracy: 0.9323\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 1s 45ms/step - loss: 0.1678 - accuracy: 0.9373 - val_loss: 0.1568 - val_accuracy: 0.9449\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 1s 40ms/step - loss: 0.1622 - accuracy: 0.9424 - val_loss: 0.1614 - val_accuracy: 0.9474\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 1s 42ms/step - loss: 0.1624 - accuracy: 0.9449 - val_loss: 0.1566 - val_accuracy: 0.9424\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 37ms/step - loss: 0.1580 - accuracy: 0.9398 - val_loss: 0.1885 - val_accuracy: 0.9273\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 1s 43ms/step - loss: 0.1817 - accuracy: 0.9298 - val_loss: 0.1588 - val_accuracy: 0.9449\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 1s 43ms/step - loss: 0.1760 - accuracy: 0.9398 - val_loss: 0.2233 - val_accuracy: 0.9248\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 39ms/step - loss: 0.2169 - accuracy: 0.9173 - val_loss: 0.2677 - val_accuracy: 0.9098\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 1s 43ms/step - loss: 0.2816 - accuracy: 0.9223 - val_loss: 0.2246 - val_accuracy: 0.9198\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 37ms/step - loss: 0.2606 - accuracy: 0.9323 - val_loss: 0.1913 - val_accuracy: 0.9348\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 36ms/step - loss: 0.2117 - accuracy: 0.9273 - val_loss: 0.2447 - val_accuracy: 0.9198\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 1s 43ms/step - loss: 0.2343 - accuracy: 0.9223 - val_loss: 0.2060 - val_accuracy: 0.9348\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 1s 44ms/step - loss: 0.2120 - accuracy: 0.9248 - val_loss: 0.1862 - val_accuracy: 0.9323\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 1s 43ms/step - loss: 0.1847 - accuracy: 0.9323 - val_loss: 0.1663 - val_accuracy: 0.9398\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1566 - accuracy: 0.9424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15662287175655365, 0.9423558712005615]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = x.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0,\n",
    "    dropout=0,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    validation_data=(x,y),\n",
    "    #validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model.evaluate(x, y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/flags_validator_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/flags_validator_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('../models/flags_validator_model')\n",
    "\n",
    "# save tokenizer to file\n",
    "f = open(\"../models/flags_validator_model/tokenizer.json\", \"w\")\n",
    "f.write(tokenizer.to_json())\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
