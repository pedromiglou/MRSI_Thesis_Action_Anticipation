{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/flags/flags.csv\", header=None)\n",
    "\n",
    "flags = set([(df[1][i],df[2][i],df[3][i]) for i in range(0,14)])\n",
    "\n",
    "colors = [\"red\",\"lightblue\",\"blue\",\"green\",\"orange\",\"yellow\",\"white\"]\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for c1 in colors:\n",
    "    x.append(c1)\n",
    "    if c1 in [x[0] for x in flags]:\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)\n",
    "    \n",
    "    for c2 in colors:\n",
    "        x.append(c1+\",\"+c2)\n",
    "        if c2 in [x[1] for x in flags if x[0]==c1]:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "        \n",
    "        for c3 in colors:\n",
    "            x.append(c1+\",\"+c2+\",\"+c3)\n",
    "            if c3 in [x[2] for x in flags if x[0]==c1 and x[1]==c2]:\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)\n",
    "\n",
    "x = np.array(x)\n",
    "\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " ...\n",
      " [7 7 5]\n",
      " [7 7 6]\n",
      " [7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20, split=\",\")\n",
    "\n",
    "# updates internal vocabulary based on the lyrics\n",
    "tokenizer.fit_on_texts(x)\n",
    "\n",
    "# transform each text in x to a sequence of tokens\n",
    "x = tokenizer.texts_to_sequences(x)\n",
    "\n",
    "x = np.array(pad_sequences(x, maxlen = 3))\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "num = len(y)\n",
    "\n",
    "idx = np.random.permutation(num)\n",
    "\n",
    "x = x[idx]\n",
    "y = y[idx]\n",
    "\n",
    "x = x.reshape((x.shape[0], x.shape[1], 1))\n",
    "\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "np.unique(y)\n",
    "\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 3, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 3, 1)        2           ['input_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 3, 1)        7169        ['layer_normalization_8[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 3, 1)         0           ['multi_head_attention_4[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TFOpLa  (None, 3, 1)        0           ['dropout_9[0][0]',              \n",
      " mbda)                                                            'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 3, 1)        2           ['tf.__operators__.add_8[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 3, 4)         8           ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 3, 4)         0           ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 3, 1)         5           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  (None, 3, 1)        0           ['conv1d_9[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_8[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 3, 1)        2           ['tf.__operators__.add_9[0][0]'] \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 3, 1)        7169        ['layer_normalization_10[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 3, 1)         0           ['multi_head_attention_5[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  (None, 3, 1)        0           ['dropout_11[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_9[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 3, 1)        2           ['tf.__operators__.add_10[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 3, 4)         8           ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 3, 4)         0           ['conv1d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 3, 1)         5           ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  (None, 3, 1)        0           ['conv1d_11[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_10[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 3, 1)        2           ['tf.__operators__.add_11[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (MultiH  (None, 3, 1)        7169        ['layer_normalization_12[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 3, 1)         0           ['multi_head_attention_6[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (TFOpL  (None, 3, 1)        0           ['dropout_13[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_11[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 3, 1)        2           ['tf.__operators__.add_12[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 3, 4)         8           ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 3, 4)         0           ['conv1d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 3, 1)         5           ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (TFOpL  (None, 3, 1)        0           ['conv1d_13[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_12[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 3, 1)        2           ['tf.__operators__.add_13[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (MultiH  (None, 3, 1)        7169        ['layer_normalization_14[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 3, 1)         0           ['multi_head_attention_7[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (TFOpL  (None, 3, 1)        0           ['dropout_15[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_13[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 3, 1)        2           ['tf.__operators__.add_14[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 3, 4)         8           ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 3, 4)         0           ['conv1d_14[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 3, 1)         5           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (TFOpL  (None, 3, 1)        0           ['conv1d_15[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_14[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 3)           0           ['tf.__operators__.add_15[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          512         ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 128)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            129         ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,385\n",
      "Trainable params: 29,385\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "13/13 [==============================] - 19s 91ms/step - loss: 0.2993 - accuracy: 0.8922 - val_loss: 0.2435 - val_accuracy: 0.9248\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.2499 - accuracy: 0.9273 - val_loss: 0.2463 - val_accuracy: 0.9248\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.2284 - accuracy: 0.9248 - val_loss: 0.2154 - val_accuracy: 0.9248\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.2168 - accuracy: 0.9248 - val_loss: 0.2045 - val_accuracy: 0.9248\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.2065 - accuracy: 0.9273 - val_loss: 0.1976 - val_accuracy: 0.9373\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.2087 - accuracy: 0.9348 - val_loss: 0.1943 - val_accuracy: 0.9273\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.1987 - accuracy: 0.9323 - val_loss: 0.2032 - val_accuracy: 0.9248\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.2028 - accuracy: 0.9223 - val_loss: 0.1916 - val_accuracy: 0.9348\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.1912 - accuracy: 0.9424 - val_loss: 0.2334 - val_accuracy: 0.9248\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.2085 - accuracy: 0.9248 - val_loss: 0.1931 - val_accuracy: 0.9248\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.2006 - accuracy: 0.9223 - val_loss: 0.1980 - val_accuracy: 0.9248\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.1929 - accuracy: 0.9298 - val_loss: 0.2037 - val_accuracy: 0.9248\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.2347 - accuracy: 0.9248 - val_loss: 0.2124 - val_accuracy: 0.9248\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.2095 - accuracy: 0.9298 - val_loss: 0.2025 - val_accuracy: 0.9273\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.2027 - accuracy: 0.9298 - val_loss: 0.1873 - val_accuracy: 0.9298\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.1992 - accuracy: 0.9323 - val_loss: 0.1841 - val_accuracy: 0.9273\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1923 - accuracy: 0.9398 - val_loss: 0.2798 - val_accuracy: 0.9248\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.2121 - accuracy: 0.9373 - val_loss: 0.1819 - val_accuracy: 0.9373\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.2058 - accuracy: 0.9323 - val_loss: 0.1789 - val_accuracy: 0.9373\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.2076 - accuracy: 0.9348 - val_loss: 0.1910 - val_accuracy: 0.9348\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.2005 - accuracy: 0.9348 - val_loss: 0.2125 - val_accuracy: 0.9373\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.2034 - accuracy: 0.9373 - val_loss: 0.1847 - val_accuracy: 0.9348\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1882 - accuracy: 0.9248 - val_loss: 0.1801 - val_accuracy: 0.9298\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1856 - accuracy: 0.9298 - val_loss: 0.1896 - val_accuracy: 0.9248\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1852 - accuracy: 0.9323 - val_loss: 0.2006 - val_accuracy: 0.9398\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.2065 - accuracy: 0.9223 - val_loss: 0.2066 - val_accuracy: 0.9323\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1939 - accuracy: 0.9298 - val_loss: 0.1837 - val_accuracy: 0.9449\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.1866 - accuracy: 0.9373 - val_loss: 0.1676 - val_accuracy: 0.9398\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1724 - accuracy: 0.9298 - val_loss: 0.1730 - val_accuracy: 0.9273\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1965 - accuracy: 0.9323 - val_loss: 0.1763 - val_accuracy: 0.9323\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1748 - accuracy: 0.9373 - val_loss: 0.1729 - val_accuracy: 0.9373\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1805 - accuracy: 0.9323 - val_loss: 0.1701 - val_accuracy: 0.9398\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.1759 - accuracy: 0.9398 - val_loss: 0.1630 - val_accuracy: 0.9373\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1800 - accuracy: 0.9298 - val_loss: 0.1861 - val_accuracy: 0.9248\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1851 - accuracy: 0.9373 - val_loss: 0.1653 - val_accuracy: 0.9348\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.1765 - accuracy: 0.9323 - val_loss: 0.1586 - val_accuracy: 0.9474\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1717 - accuracy: 0.9373 - val_loss: 0.1595 - val_accuracy: 0.9499\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 34ms/step - loss: 0.1658 - accuracy: 0.9373 - val_loss: 0.1559 - val_accuracy: 0.9398\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1632 - accuracy: 0.9373 - val_loss: 0.1589 - val_accuracy: 0.9398\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1623 - accuracy: 0.9474 - val_loss: 0.1654 - val_accuracy: 0.9474\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.1614 - accuracy: 0.9474 - val_loss: 0.1551 - val_accuracy: 0.9449\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1881 - accuracy: 0.9223 - val_loss: 0.1655 - val_accuracy: 0.9348\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1752 - accuracy: 0.9248 - val_loss: 0.1735 - val_accuracy: 0.9248\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1789 - accuracy: 0.9273 - val_loss: 0.1721 - val_accuracy: 0.9273\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1856 - accuracy: 0.9348 - val_loss: 0.1914 - val_accuracy: 0.9248\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.1715 - accuracy: 0.9398 - val_loss: 0.1537 - val_accuracy: 0.9449\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.1733 - accuracy: 0.9348 - val_loss: 0.1507 - val_accuracy: 0.9449\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1621 - accuracy: 0.9424 - val_loss: 0.1507 - val_accuracy: 0.9449\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1615 - accuracy: 0.9348 - val_loss: 0.1704 - val_accuracy: 0.9273\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.1755 - accuracy: 0.9248 - val_loss: 0.1488 - val_accuracy: 0.9524\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1698 - accuracy: 0.9398 - val_loss: 0.1570 - val_accuracy: 0.9449\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.1563 - accuracy: 0.9474 - val_loss: 0.1485 - val_accuracy: 0.9549\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.1653 - accuracy: 0.9323 - val_loss: 0.1450 - val_accuracy: 0.9499\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1670 - accuracy: 0.9373 - val_loss: 0.1500 - val_accuracy: 0.9474\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1580 - accuracy: 0.9449 - val_loss: 0.1733 - val_accuracy: 0.9474\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1665 - accuracy: 0.9373 - val_loss: 0.1519 - val_accuracy: 0.9499\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1585 - accuracy: 0.9449 - val_loss: 0.1464 - val_accuracy: 0.9524\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1534 - accuracy: 0.9499 - val_loss: 0.1453 - val_accuracy: 0.9524\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.1562 - accuracy: 0.9424 - val_loss: 0.1408 - val_accuracy: 0.9449\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.1611 - accuracy: 0.9499 - val_loss: 0.1442 - val_accuracy: 0.9499\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1593 - accuracy: 0.9524 - val_loss: 0.1472 - val_accuracy: 0.9424\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1561 - accuracy: 0.9373 - val_loss: 0.2672 - val_accuracy: 0.8772\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1709 - accuracy: 0.9424 - val_loss: 0.1708 - val_accuracy: 0.9323\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1666 - accuracy: 0.9373 - val_loss: 0.1455 - val_accuracy: 0.9474\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1630 - accuracy: 0.9373 - val_loss: 0.1434 - val_accuracy: 0.9474\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.1485 - accuracy: 0.9474 - val_loss: 0.1389 - val_accuracy: 0.9549\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1474 - accuracy: 0.9474 - val_loss: 0.1423 - val_accuracy: 0.9449\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1543 - accuracy: 0.9424 - val_loss: 0.1404 - val_accuracy: 0.9574\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.1710 - accuracy: 0.9348 - val_loss: 0.1369 - val_accuracy: 0.9599\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.1507 - accuracy: 0.9499 - val_loss: 0.1363 - val_accuracy: 0.9499\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1597 - accuracy: 0.9273 - val_loss: 0.1895 - val_accuracy: 0.9348\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1817 - accuracy: 0.9373 - val_loss: 0.1443 - val_accuracy: 0.9499\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1789 - accuracy: 0.9373 - val_loss: 0.1489 - val_accuracy: 0.9424\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1621 - accuracy: 0.9398 - val_loss: 0.1430 - val_accuracy: 0.9524\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1465 - accuracy: 0.9474 - val_loss: 0.1410 - val_accuracy: 0.9474\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1506 - accuracy: 0.9449 - val_loss: 0.1416 - val_accuracy: 0.9474\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1434 - accuracy: 0.9549 - val_loss: 0.1847 - val_accuracy: 0.9273\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1661 - accuracy: 0.9373 - val_loss: 0.1364 - val_accuracy: 0.9549\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.1464 - accuracy: 0.9499 - val_loss: 0.1354 - val_accuracy: 0.9549\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.1428 - accuracy: 0.9549 - val_loss: 0.1332 - val_accuracy: 0.9549\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.1385 - accuracy: 0.9499 - val_loss: 0.1317 - val_accuracy: 0.9549\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1480 - accuracy: 0.9474 - val_loss: 0.1459 - val_accuracy: 0.9524\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1402 - accuracy: 0.9574 - val_loss: 0.1398 - val_accuracy: 0.9499\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1559 - accuracy: 0.9449 - val_loss: 0.1933 - val_accuracy: 0.9298\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1740 - accuracy: 0.9474 - val_loss: 0.1517 - val_accuracy: 0.9499\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.1547 - accuracy: 0.9474 - val_loss: 0.1475 - val_accuracy: 0.9424\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1733 - accuracy: 0.9348 - val_loss: 0.1397 - val_accuracy: 0.9499\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1660 - accuracy: 0.9424 - val_loss: 0.1448 - val_accuracy: 0.9449\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1544 - accuracy: 0.9499 - val_loss: 0.1346 - val_accuracy: 0.9574\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.1496 - accuracy: 0.9499 - val_loss: 0.1335 - val_accuracy: 0.9549\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.1463 - accuracy: 0.9449 - val_loss: 0.1493 - val_accuracy: 0.9499\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.1317 - accuracy: 0.9549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13166435062885284, 0.9548872113227844]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = x.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0,\n",
    "    dropout=0,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    validation_data=(x,y),\n",
    "    #validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model.evaluate(x, y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/flags_validator_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/flags_validator_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('../models/flags_validator_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
