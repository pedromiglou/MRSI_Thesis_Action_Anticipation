{"class_name": "Tokenizer", "config": {"num_words": 20, "filters": "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n", "lower": true, "split": ",", "char_level": false, "oov_token": null, "document_count": 399, "word_counts": "{\"red\": 162, \"lightblue\": 162, \"blue\": 162, \"green\": 162, \"orange\": 162, \"yellow\": 162, \"white\": 162}", "word_docs": "{\"red\": 141, \"lightblue\": 141, \"blue\": 141, \"green\": 141, \"orange\": 141, \"yellow\": 141, \"white\": 141}", "index_docs": "{\"1\": 141, \"2\": 141, \"3\": 141, \"4\": 141, \"5\": 141, \"6\": 141, \"7\": 141}", "index_word": "{\"1\": \"red\", \"2\": \"lightblue\", \"3\": \"blue\", \"4\": \"green\", \"5\": \"orange\", \"6\": \"yellow\", \"7\": \"white\"}", "word_index": "{\"red\": 1, \"lightblue\": 2, \"blue\": 3, \"green\": 4, \"orange\": 5, \"yellow\": 6, \"white\": 7}"}}