\chapter{Conclusion and Future Work}
\label{chapter:conclusion}

\section{Discussion}

This document studies the problem of anticipating human actions in collaborative environments with the goal of developing an anticipatory robot controller for an assembly task. Looking at previous work found in the literature, there is a clear predominance of perception using RGB cameras with different ways of preprocessing the captured images, particularly with libraries that can detect keypoints in an image such as skeleton joints which are very important to detect human poses. Then, supervised learning techniques are used to predict the action being made and associate that information with the following action.

% \item Establish the hardware and software tools used in this dissertation. Develop an infrastructure in \acs{ros} to support a practical implementation of action anticipation in the context of \acs{hrc} with a robot controller that considers the human partner's intentions to make appropriate decisions during the execution of a sequential assembly task.

% \item Explore and apply the potentialities of the MediaPipe framework. Produce deep learning models capable of perceiving and recognizing the objects being grasped by the user by using the right-hand keypoints. Perform extensive experiments, comparing the different models developed, to demonstrate the potential and limitations of the proposed approach analyzing, in particular, the generalization performance and/or model failure both across different trials for the same user and across multiple users.

The approach adopted in this work was to perform action anticipation by recognizing the object being grasped by the user from the configuration of the user's hand. To support it, an infrastructure in \acs{ros} was developed connecting a collaborative robot and two cameras to a robot controller. From the controllers tested, the rule-based provided the most flexibility by allowing to easily switch the rules used, which associate the state of the environment to a particular action.

The proposed DL-based framework for hand-object recognition relies on the MediaPipe Hands model to predict the hand keypoints and a multi-class classifier that uses them to predict the grasped object. The study focused on the classifier's generalization ability, remarking on the importance of an effective evaluation before the system is applied in real-world scenarios. Throughout the experiments, variations in performance were observed, particularly in scenarios involving session-based testing and user-specific adaptation. The main results emphasize the importance of continuous model monitoring, retraining, and active data collection to enable the classifier to generalize effectively across diverse user behaviors and grasping patterns. Personalized modeling approaches and fine-tuning strategies may be useful to address the challenges at hand. In this context, careful consideration of dataset dimensionality is essential to optimize model performance and facilitate meaningful insights from the data. 
The findings offer valuable insights into the factors influencing the performance of the classifier and the implications for real-world applications.

\section{Future Work}

This study presents a proof-of-concept about how to perform action anticipation from a different type of information and the associated limitations. This work sets the stage for ongoing improvements with the ultimate goal of delivering effective solutions for a wide range of applications. Moving forward, there are three main topics of research:

\begin{itemize}
    \item \textbf{More Data}: test the current architectures with a bigger dataset which can have more people from different sex and/or age groups, can have more objects, can be from different keypoint detection frameworks, or can be from different camera angles.
    % \item \textbf{More Architectures}: test more neural network architectures such as Feed Forward to take advantage of the low size of the data or \acs{rnn}s which also take advantage of the data structure or try different architectures from the used neural network.
    % It seems that the problem do not lie so much in more architectures, but in the strategies to address some of the limitation, as suggested in a previous sentence:"The main results emphasize  the importance of continuous model monitoring, retraining, and active data collection to enable the classifier to generalize..."
    %\item \textbf{Adaptability Strategies}: explore different adaptation strategies to adress the observed limitations, such as continuous model monitoring, retraining, and active data collection. %to improve the generalization capabilities of the models. For example, the use of transfer learning to adapt the models to new users or the use of online learning to adapt the models to new objects.
    \item \textbf{Adaptability Strategies}: explore advanced techniques to further enhance the adaptability and generalization capabilities of the hand-object recognition system, namely by exploring data from human grasping databases like \cite{Saudabayev2018}.
    \item \textbf{Real-Time Integration}: although not tested in a specific example, most of the infrastructure needed to proceed to real-time integration was implemented. However, a real-time application of this work could prove to be an interesting research topic when dealing with the limitations of the system described in \autoref{section:human_intention_prediction}. In particular, using the models in this work alongside an object recognition model that classifies directly from images could provide a consistent object detection since they complement each other.
\end{itemize}

% Future research will explore advanced techniques to further enhance the adaptability and generalization capabilities of the hand-object recognition system, namely by exploring data from human grasping databases like \cite{Saudabayev2018}. This work sets the stage for ongoing improvements with the ultimate goal of delivering effective solutions for a wide range of applications.

\section{Contributions}

This work provided the following contributions:

\begin{itemize}
    \item P. Amaral, F. Silva, V. Santos, «Recognition of Grasping Patterns using Deep Learning for Human-Robot Colaboration», Sensors (in press).
    \item Pedro Amaral, Recognition of Human Grasping Patterns for Intention Prediction in Collaborative Tasks, Github Repository, \url{https://github.com/pedromiglou/MRSI_Thesis_Action_Anticipation}.
    \item Pedro Amaral, Human Grasping Patterns for Object Recognition, Kaggle Dataset, \textcolor{red}{\url{...}}.
\end{itemize}
