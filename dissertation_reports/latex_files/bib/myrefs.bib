@book{Braitenberg1986,
    author    = "Valentino Braitenberg",
    title     = "Vehicles: Experiments in Synthetic Psychology",
    publisher = "The MIT Press",
    year      = "1986",
    month    = "2",
    isbn = {9780262521123},
}
@article{Sebanz2006,
   abstract = {The ability to coordinate our actions with those of others is crucial for our success as individuals and as a species. Progress in understanding the cognitive and neural processes involved in joint action has been slow and sparse, because cognitive neuroscientists have predominantly studied individual minds and brains in isolation. However, in recent years, major advances have been made by investigating perception and action in social context. In this article we outline how studies on joint attention, action observation, task sharing, action coordination and agency contribute to the understanding of the cognitive and neural processes supporting joint action. Several mechanisms are proposed that allow individuals to share representations, to predict actions, and to integrate predicted effects of own and others' actions. © 2005 Elsevier Ltd. All rights reserved.},
   author = {Natalie Sebanz and Harold Bekkering and Günther Knoblich},
   doi = {10.1016/j.tics.2005.12.009},
   issn = {13646613},
   issue = {2},
   journal = {Trends in Cognitive Sciences},
   month = {2},
   pages = {70-76},
   pmid = {16406326},
   title = {Joint action: bodies and minds moving together},
   volume = {10},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661305003566},
   year = {2006},
}
@article{Hoffman2007,
   abstract = {A crucial skill for fluent action meshing in human team activity is a learned and calculated selection of anticipatory actions. We believe that the same holds for robotic teammates, if they are to perform in a similarly fluent manner with their human counterparts. In this work, we describe a model for human-robot joint action, and propose an adaptive action selection mechanism for a robotic teammate, which makes anticipatory decisions based on the confidence of their validity and their relative risk. We conduct an analysis of our method, predicting an improvement in task efficiency compared to a purely reactive process. We then present results from a study involving untrained human subjects working with a simulated version of a robot using our system. We show a significant improvement in best-case task efficiency when compared to a group of users working with a reactive agent, as well as a significant difference in the perceived commitment of the robot to the team and its contribution to the team's fluency and success. By way of explanation, we raise a number of fluency metric hypotheses, and evaluate their significance between the two study conditions. © 2007 IEEE.},
   author = {Guy Hoffman and Cynthia Breazeal},
   doi = {10.1109/TRO.2007.907483},
   issn = {1552-3098},
   issue = {5},
   journal = {IEEE Transactions on Robotics},
   keywords = {Algorithms,Anticipatory action selection,Fluency,Human factors,Human-robot interaction,Teamwork},
   month = {10},
   pages = {952-961},
   title = {Cost-Based Anticipatory Action Selection for Human–Robot Fluency},
   volume = {23},
   url = {https://ieeexplore.ieee.org/document/4339531/},
   year = {2007},
}
@article{Wang2017,
   abstract = {Anticipation can enhance the capability of a robot in its interaction with humans, where the robot predicts the humans' intention for selecting its own action. We present a novel framework of anticipatory action selection for human–robot interaction, which is capable to handle nonlinear and stochastic human behaviors such as table tennis strokes and allows the robot to choose the optimal action based on prediction of the human partner's intention with uncertainty. The presented framework is generic and can be used in many human–robot interaction scenarios, for example, in navigation and human–robot co-manipulation. In this article, we conduct a case study on human–robot table tennis. Due to the limited amount of time for executing hitting movements, a robot usually needs to initiate its hitting movement before the opponent hits the ball, which requires the robot to be anticipatory based on visual observation of the opponent's movement. Previous work on Intention-Driven Dynamics Models (IDDM) allowed the robot to predict the intended target of the opponent. In this article, we address the problem of action selection and optimal timing for initiating a chosen action by formulating the anticipatory action selection as a Partially Observable Markov Decision Process (POMDP), where the transition and observation are modeled by the IDDM framework. We present two approaches to anticipatory action selection based on the POMDP formulation, i.e., a model-free policy learning method based on Least-Squares Policy Iteration (LSPI) that employs the IDDM for belief updates, and a model-based Monte-Carlo Planning (MCP) method, which benefits from the transition and observation model by the IDDM. Experimental results using real data in a simulated environment show the importance of anticipatory action selection, and that POMDPs are suitable to formulate the anticipatory action selection problem by taking into account the uncertainties in prediction. We also show that existing algorithms for POMDPs, such as LSPI and MCP, can be applied to substantially improve the robot's performance in its interaction with humans.},
   author = {Zhikun Wang and Abdeslam Boularias and Katharina Mülling and Bernhard Schölkopf and Jan Peters},
   doi = {10.1016/j.artint.2014.11.007},
   issn = {00043702},
   journal = {Artificial Intelligence},
   keywords = {Anticipation,Intention-driven dynamics model,Monte-Carlo planning,Partially observable Markov decision process,Policy iteration},
   month = {6},
   pages = {399-414},
   publisher = {Elsevier B.V.},
   title = {Anticipatory action selection for human–robot table tennis},
   volume = {247},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370214001398},
   year = {2017},
}
@article{Carneiro2021,
   abstract = {<p>Catching flying objects is a challenging task in human–robot interaction. Traditional techniques predict the intersection position and time using the information obtained during the free-flying ball motion. A common pain point in these systems is the short ball flight time and uncertainties in the ball’s trajectory estimation. In this paper, we present the Robot Anticipation Learning System (RALS) that accounts for the information obtained from observation of the thrower’s hand motion before the ball is released. RALS takes extra time for the robot to start moving in the direction of the target before the opponent finishes throwing. To the best of our knowledge, this is the first robot control system for ball-catching with anticipation skills. Our results show that the information fused from both throwing and flying motions improves the ball-catching rate by up to 20\% compared to the baseline approach, with the predictions relying only on the information acquired during the flight phase.</p>},
   author = {Diogo Carneiro and Filipe Silva and Petia Georgieva},
   doi = {10.3390/robotics10040113},
   issn = {2218-6581},
   issue = {4},
   journal = {Robotics},
   keywords = {Anticipation learning,Ball catching,Human–robot interaction,Neural network,Trajectory prediction},
   month = {10},
   pages = {113},
   publisher = {MDPI},
   title = {Robot Anticipation Learning System for Ball Catching},
   volume = {10},
   url = {https://www.mdpi.com/2218-6581/10/4/113},
   year = {2021},
}
@article{Deans2021,
   abstract = {<p>Anticipation is the act of using information about the past and present to make predictions about future scenarios. As a concept, it is predominantly associated with the psychology of the human mind; however, there is accumulating evidence that diverse taxa without complex neural systems, and even biochemical networks themselves, can respond to perceived future conditions. Although anticipatory processes, such as circadian rhythms, stress priming, and cephalic responses, have been extensively studied over the last three centuries, newer research on anticipatory genetic networks in microbial species shows that anticipatory processes are widespread, evolutionarily old, and not simply reserved for neurological complex organisms. Overall, data suggest that anticipatory responses represent a unique type of biological processes that can be distinguished based on their organizational properties and mechanisms. Unfortunately, an empirically based biologically explicit framework for describing anticipatory processes does not currently exist. This review attempts to fill this void by discussing the existing examples of anticipatory processes in non-cognitive organisms, providing potential criteria for defining anticipatory processes, as well as their putative mechanisms, and drawing attention to the often-overlooked role of anticipation in the evolution of physiological systems. Ultimately, a case is made for incorporating an anticipatory framework into the existing physiological paradigm to advance our understanding of complex biological processes.</p>},
   author = {Carrie Deans},
   doi = {10.3389/fphys.2021.672457},
   issn = {1664-042X},
   journal = {Frontiers in Physiology},
   keywords = {allostasis,cephalic responses,feed-forward control,microbe,non-cognitive,physiological regulation,prediction},
   month = {12},
   publisher = {Frontiers Media S.A.},
   title = {Biological Prescience: The Role of Anticipation in Organismal Processes},
   volume = {12},
   url = {https://www.frontiersin.org/articles/10.3389/fphys.2021.672457/full},
   year = {2021},
}
@unpublished{Ahmed2020,
   abstract = {Despite recent successes of reinforcement learning (RL), it remains a challenge for agents to transfer learned skills to related environments. To facilitate research addressing this problem, we propose CausalWorld, a benchmark for causal structure and transfer learning in a robotic manipulation environment. The environment is a simulation of an open-source robotic platform, hence offering the possibility of sim-to-real transfer. Tasks consist of constructing 3D shapes from a given set of blocks - inspired by how children learn to build complex structures. The key strength of CausalWorld is that it provides a combinatorial family of such tasks with common causal structure and underlying factors (including, e.g., robot and object masses, colors, sizes). The user (or the agent) may intervene on all causal variables, which allows for fine-grained control over how similar different tasks (or task distributions) are. One can thus easily define training and evaluation distributions of a desired difficulty level, targeting a specific form of generalization (e.g., only changes in appearance or object mass). Further, this common parametrization facilitates defining curricula by interpolating between an initial and a target task. While users may define their own task distributions, we present eight meaningful distributions as concrete benchmarks, ranging from simple to very challenging, all of which require long-horizon planning as well as precise low-level motor control. Finally, we provide baseline results for a subset of these tasks on distinct training curricula and corresponding evaluation protocols, verifying the feasibility of the tasks in this benchmark.},
   author = {Ossama Ahmed and Frederik Träuble and Anirudh Goyal and Alexander Neitz and Yoshua Bengio and Bernhard Schölkopf and Manuel Wüthrich and Stefan Bauer},
   month = {10},
   title = {CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning},
   url = {http://arxiv.org/abs/2010.04296},
   year = {2020},
   note = {unpublished},
}
@article{Semeraro2023,
   abstract = {Technological progress increasingly envisions the use of robots interacting with people in everyday life. Human–robot collaboration (HRC) is the approach that explores the interaction between a human and a robot, during the completion of a common objective, at the cognitive and physical level. In HRC works, a cognitive model is typically built, which collects inputs from the environment and from the user, elaborates and translates these into information that can be used by the robot itself. Machine learning is a recent approach to build the cognitive model and behavioural block, with high potential in HRC. Consequently, this paper proposes a thorough literature review of the use of machine learning techniques in the context of human–robot collaboration. 45 key papers were selected and analysed, and a clustering of works based on the type of collaborative tasks, evaluation metrics and cognitive variables modelled is proposed. Then, a deep analysis on different families of machine learning algorithms and their properties, along with the sensing modalities used, is carried out. Among the observations, it is outlined the importance of the machine learning algorithms to incorporate time dependencies. The salient features of these works are then cross-analysed to show trends in HRC and give guidelines for future works, comparing them with other aspects of HRC not appeared in the review.},
   author = {Francesco Semeraro and Alexander Griffiths and Angelo Cangelosi},
   doi = {10.1016/j.rcim.2022.102432},
   issn = {07365845},
   journal = {Robotics and Computer-Integrated Manufacturing},
   month = {2},
   pages = {102432},
   title = {Human–robot collaboration and machine learning: A systematic review of recent research},
   volume = {79},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0736584522001156},
   year = {2023},
}
@article{Zhang2022,
   abstract = {<p>Most robots are programmed to carry out specific tasks routinely with minor variations. However, more and more applications from SMEs require robots work alongside their counterpart human workers. To smooth the collaboration task flow and improve the collaboration efficiency, a better way is to formulate the robot to surmise what kind of assistance a human coworker needs and naturally take the right action at the right time. This paper proposes a prediction-based human-robot collaboration model for assembly scenarios. An embedded learning from demonstration technique enables the robot to understand various task descriptions and customized working preferences. A state-enhanced convolutional long short-term memory (ConvLSTM)-based framework is formulated for extracting the high-level spatiotemporal features from the shared workspace and predicting the future actions to facilitate the fluent task transition. This model allows the robot to adapt itself to predicted human actions and enables proactive assistance during collaboration. We applied our model to the seats assembly experiment for a scale model vehicle and it can obtain a human worker’s intentions, predict a coworker’s future actions, and provide assembly parts correspondingly. It has been verified that the proposed framework yields higher smoothness and shorter idle times, and meets more working styles, compared to the state-of-the-art methods without prediction awareness.</p>},
   author = {Zhujun Zhang and Gaoliang Peng and Weitian Wang and Yi Chen and Yunyi Jia and Shaohui Liu},
   doi = {10.3390/s22114279},
   issn = {1424-8220},
   issue = {11},
   journal = {Sensors},
   month = {6},
   pages = {4279},
   title = {Prediction-Based Human-Robot Collaboration in Assembly Tasks Using a Learning from Demonstration Model},
   volume = {22},
   url = {https://www.mdpi.com/1424-8220/22/11/4279},
   year = {2022},
}
@article{Mukherjee2022,
   abstract = {Increased global competition has placed a premium on customer satisfaction, and there is a greater demand for manufacturers to be flexible with their products and services. This challenge is usually addressed with the introduction of human operators for precise tasks that require dexterity, flexibility and cognitive decision making. On the other hand, robots, through automation, are very effective in carrying out repetitive, non-ergonomic tasks. Owing to the complementary nature of robots’ and humans’ capabilities, there is an increased interest towards a shared workspace for humans and robots to work together collaboratively, forming the motivation behind the field of human-robot collaboration (HRC). Research in HRC in industry is concerned with the safety of the humans and robots, extent, and modes of collaboration among them, and the level of autonomy and adaptability of robots that can be trained for different tasks. This paper introduces a novel taxonomy of levels of interaction between humans and robots along the lines of SAEs guidelines for autonomous vehicles in response to a need for standard definitions and evolving nature of the field. Research into modes of communication for HRC driven by machine learning are reviewed followed by broad definitions of the types of machine learning. The authors also present a comprehensive review of the machine learning (ML) methodologies and industrial applications of the same in the context of adaptable collaborative robots.},
   author = {Debasmita Mukherjee and Kashish Gupta and Li Hsin Chang and Homayoun Najjaran},
   doi = {10.1016/j.rcim.2021.102231},
   issn = {07365845},
   journal = {Robotics and Computer-Integrated Manufacturing},
   month = {2},
   pages = {102231},
   title = {A Survey of Robot Learning Strategies for Human-Robot Collaboration in Industrial Settings},
   volume = {73},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0736584521001137},
   year = {2022},
}
@inproceedings{Schydlo2018,
   abstract = {Close human-robot cooperation is a key enabler for new developments in advanced manufacturing and assistive applications. Close cooperation require robots that can predict human actions and intent, understanding human non-verbal cues. Recent approaches based on neural networks have led to encouraging results in the human action prediction problem both in continuous and discrete spaces. Our approach extends the research in this direction. Our contributions are three-fold. First, we validate the use of gaze and body pose cues as a means of predicting human action through a feature selection method. Next, we address two shortcomings of existing literature: predicting multiple and variable-length action sequences. This is achieved by applying an encoder-decoder recurrent neural network topology in the discrete action prediction problem. In addition, we theoretically demonstrate the importance of predicting multiple action sequences as a means of estimating the stochastic reward in a human robot cooperation scenario. Finally, we show the ability to effectively train the prediction model on an action prediction dataset, involving human motion data, and explore the influence of the model's parameters on its performance.},
   author = {Paul Schydlo and Mirko Rakovic and Lorenzo Jamone and Jose Santos-Victor},
   doi = {10.1109/ICRA.2018.8460924},
   isbn = {978-1-5386-3081-5},
   issn = {10504729},
   journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
   month = {5},
   pages = {1-6},
   publisher = {IEEE},
   title = {Anticipation in Human-Robot Cooperation: A Recurrent Neural Network Approach for Multiple Action Sequences Prediction},
   url = {https://ieeexplore.ieee.org/document/8460924/},
   year = {2018},
}
@inproceedings{Maeda2016,
   abstract = {This paper introduces our initial investigation on the problem of providing a semi-autonomous robot collaborator with anticipative capabilities to predict human actions. Anticipative robot behavior is a desired characteristic of robot collaborators that lead to fluid, proactive interactions. We are particularly interested in improving reactive methods that rely on human action recognition to activate the corresponding robot action. Action recognition invariably causes delay in the robot's response, and the goal of our method is to eliminate this delay by predicting the next human action. Prediction is achieved by using a lookup table containing variations of assembly sequences, previously demonstrated by different users. The method uses the nearest neighbor sequence in the table that matches the actual sequence of human actions. At the movement level, our method uses a probabilistic representation of interaction primitives to generate robot trajectories. The method is demonstrated using a 7 degree-of-freedom lightweight arm equipped with a 5-finger hand on an assembly task consisting of 17 steps.},
   author = {Guilherme J. Maeda and Aayushi Maloo and Marco Ewerton and Rudolf Lioutikov and Jan Peters},
   isbn = {9781577357759},
   booktitle = {AAAI Fall Symposium - Technical Report},
   title = {Anticipative interaction primitives for human-robot collaboration},
   year = {2016},
}
@article{Furnari2021,
   abstract = {In this paper, we tackle the problem of egocentric action anticipation, i.e., predicting what actions the camera wearer will perform in the near future and which objects they will interact with. Specifically, we contribute Rolling-Unrolling LSTM, a learning architecture to anticipate actions from egocentric videos. The method is based on three components: 1) an architecture comprised of two LSTMs to model the sub-tasks of summarizing the past and inferring the future, 2) a Sequence Completion Pre-Training technique which encourages the LSTMs to focus on the different sub-tasks, and 3) a Modality ATTention (MATT) mechanism to efficiently fuse multi-modal predictions performed by processing RGB frames, optical flow fields and object-based features. The proposed approach is validated on EPIC-Kitchens, EGTEA Gaze+ and ActivityNet. The experiments show that the proposed architecture is state-of-the-art in the domain of egocentric videos, achieving top performances in the 2019 EPIC-Kitchens egocentric action anticipation challenge. The approach also achieves competitive performance on ActivityNet with respect to methods not based on unsupervised pre-training and generalizes to the tasks of early action recognition and action recognition. To encourage research on this challenging topic, we made our code, trained models, and pre-extracted features available at our web page: http://iplab.dmi.unict.it/rulstm.},
   author = {Antonino Furnari and Giovanni Maria Farinella},
   doi = {10.1109/TPAMI.2020.2992889},
   issn = {0162-8828},
   issue = {11},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   keywords = {Action anticipation,LSTM,egocentric vision,recurrent neural networks},
   month = {11},
   pages = {4021-4036},
   pmid = {32386143},
   title = {Rolling-Unrolling LSTMs for Action Anticipation from First-Person Video},
   volume = {43},
   url = {https://ieeexplore.ieee.org/document/9088213/},
   year = {2021},
}
@article{Canuto2021,
   abstract = {To interact with humans in collaborative environments, machines need to be able to predict (i.e., anticipate) future events, and execute actions in a timely manner. However, the observation of the human limb movements may not be sufficient to anticipate their actions unambiguously. In this work, we consider two additional sources of information (i.e., context) over time, gaze, movement and object information, and study how these additional contextual cues improve the action anticipation performance. We address action anticipation as a classification task, where the model takes the available information as the input and predicts the most likely action. We propose to use the uncertainty about each prediction as an online decision-making criterion for action anticipation. Uncertainty is modeled as a stochastic process applied to a time-based neural network architecture, which improves the conventional class-likelihood (i.e., deterministic) criterion. The main contributions of this paper are fourfold: (i) We propose a novel and effective decision-making criterion that can be used to anticipate actions even in situations of high ambiguity; (ii) we propose a deep architecture that outperforms previous results in the action anticipation task when using the Acticipate collaborative dataset; (iii) we show that contextual information is important to disambiguate the interpretation of similar actions; and (iv) we also provide a formal description of three existing performance metrics that can be easily used to evaluate action anticipation models. Our results on the Acticipate dataset showed the importance of contextual information and the uncertainty criterion for action anticipation. We achieve an average accuracy of 98.75\% in the anticipation task using only an average of 25\% of observations. Also, considering that a good anticipation model should perform well in the action recognition task, we achieve an average accuracy of 100\% in action recognition on the Acticipate dataset, when the entire observation set is used.},
   author = {Clebeson Canuto and Plinio Moreno and Jorge Samatelo and Raquel Vassallo and José Santos-Victor},
   doi = {10.1016/j.neucom.2020.07.135},
   issn = {09252312},
   journal = {Neurocomputing},
   keywords = {Action anticipation,Bayesian deep learning,Context information,Early action prediction,Uncertainty},
   month = {7},
   pages = {301-318},
   title = {Action anticipation for collaborative environments: The impact of contextual information and uncertainty-based prediction},
   volume = {444},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220317719},
   year = {2021},
}
@article{Wang2021,
   abstract = {Video action anticipation aims to predict future action categories from observed frames. Current state-of-the-art approaches mainly resort to recurrent neural networks to encode history information into hidden states, and predict future actions from the hidden representations. It is well known that the recurrent pipeline is inefficient in capturing long-term information which may limit its performance in predication task. To address this problem, this paper proposes a simple yet efficient Temporal Transformer with Progressive Prediction (TTPP) framework, which repurposes a Transformer-style architecture to aggregate observed features, and then leverages a light-weight network to progressively predict future features and actions. Specifically, predicted features along with predicted probabilities are accumulated into the inputs of subsequent prediction. We evaluate our approach on three action datasets, namely TVSeries, THUMOS-14, and TV-Human-Interaction. Additionally we also conduct a comprehensive study for several popular aggregation and prediction strategies. Extensive results show that TTPP not only outperforms the state-of-the-art methods but also more efficient.},
   author = {Wen Wang and Xiaojiang Peng and Yanzhou Su and Yu Qiao and Jian Cheng},
   doi = {10.1016/j.neucom.2021.01.087},
   issn = {09252312},
   journal = {Neurocomputing},
   keywords = {Action anticipation,Encoder-decoder,Progressive prediction,Transformer},
   month = {5},
   pages = {270-279},
   title = {TTPP: Temporal Transformer with Progressive Prediction for efficient action anticipation},
   volume = {438},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221001697},
   year = {2021},
}
@incollection{Rodriguez2019,
   abstract = {Human action-anticipation methods predict what is the future action by observing only a few portion of an action in progress. This is critical for applications where computers have to react to human actions as early as possible such as autonomous driving, human-robotic interaction, assistive robotics among others. In this paper, we present a method for human action anticipation by predicting the most plausible future human motion. We represent human motion using Dynamic Images [1] and make use of tailored loss functions to encourage a generative model to produce accurate future motion prediction. Our method outperforms the currently best performing action-anticipation methods by 4\% on JHMDB-21, 5.2\% on UT-Interaction and 5.1\% on UCF 101-24 benchmarks.},
   author = {Cristian Rodriguez and Basura Fernando and Hongdong Li},
   doi = {10.1007/978-3-030-11015-4_10},
   isbn = {9783030110147},
   issn = {16113349},
   booktitle = {Computer Vision – ECCV 2018 Workshops},
   publisher = {Springer},
   keywords = {Action-anticipation,Dynamic image,Generation,Motion representation,Prediction},
   pages = {89-105},
   title = {Action Anticipation by Predicting Future Dynamic Images},
   url = {http://link.springer.com/10.1007/978-3-030-11015-4_10},
   year = {2019},
}
@inproceedings{Gammulle2019,
   abstract = {Inspired by human neurological structures for action anticipation, we present an action anticipation model that enables the prediction of plausible future actions by forecasting both the visual and temporal future. In contrast to current state-of-the-art methods which first learn a model to predict future video features and then perform action anticipation using these features, the proposed framework jointly learns to perform the two tasks, future visual and temporal representation synthesis, and early action anticipation. The joint learning framework ensures that the predicted future embeddings are informative to the action anticipation task. Furthermore, through extensive experimental evaluations we demonstrate the utility of using both visual and temporal semantics of the scene, and illustrate how this representation synthesis could be achieved through a recurrent Generative Adversarial Network (GAN) framework. Our model outperforms the current state-of-the-art methods on multiple datasets: UCF101, UCF101-24, UT-Interaction and TV Human Interaction.},
   author = {Harshala Gammulle and Simon Denman and Sridha Sridharan and Clinton Fookes},
   doi = {10.1109/ICCV.2019.00566},
   isbn = {978-1-7281-4803-8},
   issn = {15505499},
   booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
   month = {10},
   pages = {5561-5570},
   publisher = {IEEE},
   title = {Predicting the Future: A Jointly Learnt Model for Action Anticipation},
   url = {https://ieeexplore.ieee.org/document/9009844/},
   year = {2019},
}
@article{Wu2021,
   abstract = {Anticipating actions before they are executed is crucial for a wide range of practical applications, including autonomous driving and robotics. In this paper, we study the egocentric action anticipation task, which predicts future action seconds before it is performed for egocentric videos. Previous approaches focus on summarizing the observed content and directly predicting future action based on past observations. We believe it would benefit the action anticipation if we could mine some cues to compensate for the missing information of the unobserved frames. We then propose to decompose the action anticipation into a series of future feature predictions. We imagine how the visual feature changes in the near future and then predicts future action labels based on these imagined representations. Differently, our ImagineRNN is optimized in a contrastive learning way instead of feature regression. We utilize a proxy task to train the ImagineRNN, i.e., selecting the correct future states from distractors. We further improve ImagineRNN by residual anticipation, i.e., changing its target to predicting the feature difference of adjacent frames instead of the frame content. This promotes the network to focus on our target, i.e., the future action, as the difference between adjacent frame features is more important for forecasting the future. Extensive experiments on two large-scale egocentric action datasets validate the effectiveness of our method. Our method significantly outperforms previous methods on both the seen test set and the unseen test set of the EPIC Kitchens Action Anticipation Challenge.},
   author = {Yu Wu and Linchao Zhu and Xiaohan Wang and Yi Yang and Fei Wu},
   doi = {10.1109/TIP.2020.3040521},
   issn = {1057-7149},
   journal = {IEEE Transactions on Image Processing},
   keywords = {Action anticipation,action prediction,egocentric videos},
   pages = {1143-1152},
   pmid = {33270562},
   title = {Learning to Anticipate Egocentric Actions by Imagination},
   volume = {30},
   url = {https://ieeexplore.ieee.org/document/9280353/},
   year = {2021},
}
@article{Moutinho2023,
   abstract = {Human–Robot Collaboration is a critical component of Industry 4.0, contributing to a transition towards more flexible production systems that are quickly adjustable to changing production requirements. This paper aims to increase the natural collaboration level of a robotic engine assembly station by proposing a cognitive system powered by computer vision and deep learning to interpret implicit communication cues of the operator. The proposed system, which is based on a residual convolutional neural network with 34 layers and a long-short term memory recurrent neural network (ResNet-34 + LSTM), obtains assembly context through action recognition of the tasks performed by the operator. The assembly context was then integrated in a collaborative assembly plan capable of autonomously commanding the robot tasks. The proposed model showed a great performance, achieving an accuracy of 96.65\% and a temporal mean intersection over union (mIoU) of 94.11\% for the action recognition of the considered assembly. Moreover, a task-oriented evaluation showed that the proposed cognitive system was able to leverage the performed human action recognition to command the adequate robot actions with near-perfect accuracy. As such, the proposed system was considered as successful at increasing the natural collaboration level of the considered assembly station.},
   author = {Duarte Moutinho and Luís F. Rocha and Carlos M. Costa and Luís F. Teixeira and Germano Veiga},
   doi = {10.1016/j.rcim.2022.102449},
   issn = {07365845},
   journal = {Robotics and Computer-Integrated Manufacturing},
   keywords = {Deep learning,Human action recognition,Human-robot collaboration},
   month = {4},
   pages = {102449},
   publisher = {Elsevier Ltd},
   title = {Deep learning-based human action recognition to leverage context awareness in collaborative assembly},
   volume = {80},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0736584522001314},
   year = {2023},
}
@article{Yu2022,
   abstract = {In today's manufacturing system, robots are expected to perform increasingly complex manipulation tasks in collaboration with humans. However, current industrial robots are still largely preprogrammed with very little autonomy and still required to be reprogramed by robotics experts for even slightly changed tasks. Therefore, it is highly desirable that robots can adapt to certain task changes with motion planning strategies to easily work with non-robotic experts in manufacturing environments. In this paper, we propose a user-guided motion planning algorithm in combination with reinforcement learning (RL) method to enable robots automatically generate their motion plans for new tasks by learning from a few kinesthetic human demonstrations. Features of common human demonstrated tasks in a specific application environment, e.g., desk assembly or warehouse loading/unloading are abstracted and saved in a library. The definition of semantical similarity between features in the library and features of a new task is proposed and further used to construct the reward function in RL. To achieve an adaptive motion plan facing task changes or new task requirements, features embedded in the library are mapped to appropriate task segments based on the trained motion planning policy using Q-learning. A new task can be either learned as a combination of a few features in the library or a requirement for further human demonstration if the current library is insufficient for the new task. We evaluate our approach on a 6 DOF UR5e robot on multiple tasks and scenarios and show the effectiveness of our method with respect to different scenarios.},
   author = {Tian Yu and Qing Chang},
   doi = {10.1016/j.eswa.2022.118291},
   issn = {09574174},
   journal = {Expert Systems with Applications},
   keywords = {Human-robot collaboration,Learning from demonstration,Motion planning,Reinforcement learning},
   month = {12},
   pages = {118291},
   publisher = {Elsevier Ltd},
   title = {User-guided motion planning with reinforcement learning for human-robot collaboration in smart manufacturing},
   volume = {209},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417422014270},
   year = {2022},
}
@misc{CobotsWW,
   author = {WiredWorkers},
   title = {Cobots},
   note = {Last accessed 3 January 2023},
   url = {https://wiredworkers.io/cobot/},
}
@unpublished{Geest2016,
   abstract = {In online action detection, the goal is to detect the start of an action in a video stream as soon as it happens. For instance, if a child is chasing a ball, an autonomous car should recognize what is going on and respond immediately. This is a very challenging problem for four reasons. First, only partial actions are observed. Second, there is a large variability in negative data. Third, the start of the action is unknown, so it is unclear over what time window the information should be integrated. Finally, in real world data, large within-class variability exists. This problem has been addressed before, but only to some extent. Our contributions to online action detection are threefold. First, we introduce a realistic dataset composed of 27 episodes from 6 popular TV series. The dataset spans over 16 hours of footage annotated with 30 action classes, totaling 6,231 action instances. Second, we analyze and compare various baseline methods, showing this is a challenging problem for which none of the methods provides a good solution. Third, we analyze the change in performance when there is a variation in viewpoint, occlusion, truncation, etc. We introduce an evaluation protocol for fair comparison. The dataset, the baselines and the models will all be made publicly available to encourage (much needed) further research on online action detection on realistic data.},
   author = {Roeland De Geest and Efstratios Gavves and Amir Ghodrati and Zhenyang Li and Cees Snoek and Tinne Tuytelaars},
   month = {4},
   title = {Online Action Detection},
   url = {http://arxiv.org/abs/1604.06506},
   year = {2016},
   note = {unpublished},
}
@article{Cao2021,
   abstract = {Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, PAFs and body part location estimation were refined simultaneously across training stages. We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.},
   author = {Zhe Cao and Gines Hidalgo and Tomas Simon and Shih-En Wei and Yaser Sheikh},
   doi = {10.1109/TPAMI.2019.2929257},
   issn = {0162-8828},
   issue = {1},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   keywords = {2D foot keypoint estimation,2D human pose estimation,multiple person,part affinity fields,real-time},
   month = {1},
   pages = {172-186},
   pmid = {31331883},
   publisher = {IEEE Computer Society},
   title = {OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields},
   volume = {43},
   url = {https://ieeexplore.ieee.org/document/8765346/},
   year = {2021},
}
@unpublished{Cao2018,
   abstract = {Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, PAFs and body part location estimation were refined simultaneously across training stages. We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.},
   author = {Zhe Cao and Gines Hidalgo and Tomas Simon and Shih-En Wei and Yaser Sheikh},
   month = {12},
   title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
   url = {http://arxiv.org/abs/1812.08008},
   year = {2018},
   note = {unpublished}
}
@unpublished{Wei2016,
   abstract = {Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.},
   author = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},
   month = {1},
   title = {Convolutional Pose Machines},
   url = {http://arxiv.org/abs/1602.00134},
   year = {2016},
   note = {unpublished}
}
@unpublished{Simon2017,
   abstract = {We present an approach that uses a multi-camera system to train fine-grained detectors for keypoints that are prone to occlusion, such as the joints of a hand. We call this procedure multiview bootstrapping: first, an initial keypoint detector is used to produce noisy labels in multiple views of the hand. The noisy detections are then triangulated in 3D using multiview geometry or marked as outliers. Finally, the reprojected triangulations are used as new labeled training data to improve the detector. We repeat this process, generating more labeled data in each iteration. We derive a result analytically relating the minimum number of views to achieve target true and false positive rates for a given detector. The method is used to train a hand keypoint detector for single images. The resulting keypoint detector runs in realtime on RGB images and has accuracy comparable to methods that use depth sensors. The single view detector, triangulated over multiple views, enables 3D markerless hand motion capture with complex object interactions.},
   author = {Tomas Simon and Hanbyul Joo and Iain Matthews and Yaser Sheikh},
   month = {4},
   title = {Hand Keypoint Detection in Single Images using Multiview Bootstrapping},
   url = {http://arxiv.org/abs/1704.07809},
   year = {2017},
   note = {unpublished}
}
@book{Rosen1985,
   author = {Robert Rosen},
   doi = {10.1016/C2009-0-07769-1},
   isbn = {9780080311586},
   pages = {339-347},
   publisher = {Elsevier},
   title = {Anticipatory Systems: Philosophical, Mathematical and Methodological Foundations},
   year = {1985},
}
@article{Poli2010,
   abstract = {Purpose – After summarizing the theories of anticipation proposed over the past century, the paper aims to distinguish between anticipation as an empirical phenomenon and the conditions that make anticipation possible. The paper's first part seeks to show that many scholars from various research fields worked on the many nuances of anticipation. The paper's second part seeks to discuss the difference between the capacity of anticipation and the nature of systems able to exhibit anticipatory behavior. The former endeavor adopts a descriptive attitude, whilst the latter seeks to understand what it is that makes anticipation possible. Design/methodology/approach – The paper presents a theoretical and experimental analysis of anticipation and anticipatory systems. Findings – Anticipation is a widely studied phenomenon within a number of different disciplines, including biology and brain studies, cognitive and social sciences, engineering and artificial intelligence. There is a need for relying on at least two different levels of analysis, namely anticipation as an empirical phenomenon and the idea of an anticipatory system or the study of the internal structure that a system should possess so that it can behave in an anticipatory fashion. Research limitations/implications – The literature summarized by the paper is only part of a substantially larger body of documents. More extensive analyses are needed to firmly establish the conclusions suggested. Practical implications – The paper allows better understanding of the complexity of anticipation and the differences between types of anticipation (e.g. between explicit versus implicit anticipation). Originality/value – For the first time, the distinction implicitly present in the surveyed literature between anticipation as an empirical phenomenon and the idea of anticipatory system as the study of the conditions that make anticipation possible is raised explicitly. © 2010, Emerald Group Publishing Limited},
   author = {Roberto Poli},
   doi = {10.1108/14636681011049839},
   editor = {Riel Miller},
   issn = {1463-6689},
   issue = {3},
   journal = {Foresight},
   keywords = {Complexity theory,Forecasting,Philosophical concepts},
   month = {6},
   pages = {7-17},
   title = {The many aspects of anticipation},
   volume = {12},
   url = {https://www.emerald.com/insight/content/doi/10.1108/14636681011049839/full/html},
   year = {2010},
}
@article{Louie2010,
   abstract = {Purpose: This article aims to be an expository introduction to Robert Rosen's anticipatory systems, the theory of which provides the conceptual basis for foresight studies. Design/methodology/approach: The ubiquity of anticipatory systems in nature is explained. Findings: Causality is not violated by anticipatory systems, and teleology is an integral aspect of science. Practical implications: A terse exposition for a general readership, such as the present article, by definition cannot get into too many details. For further exploration the reader is referred to the recent book More than Life Itself by the author. Originality/value: The topic of anticipatory systems in particular, and methods of relational biology in general, provide important tools for foresight studies. It is the author's hope that this brief glimpse into the world of relational biology piques the interest of some readers to pursue the subject further. © Emerald Group Publishing Limited.},
   author = {A.H. Louie},
   doi = {10.1108/14636681011049848},
   editor = {Riel Miller},
   issn = {1463-6689},
   issue = {3},
   journal = {Foresight},
   keywords = {Biology,Philosophical concepts,Research},
   month = {6},
   pages = {18-29},
   title = {Robert Rosen's anticipatory systems},
   volume = {12},
   url = {https://www.emerald.com/insight/content/doi/10.1108/14636681011049848/full/html},
   year = {2010},
}
@article{Smith2016,
   abstract = {The purpose of this study was to provide a systematic review of action anticipation studies using functional neuroimaging or brain stimulation during a sport-specific anticipation task. A total of 15 studies from 2008 to 2014 were evaluated and are reported in four sections: expert-novice samples, action anticipation tasks, neuroimaging and stimulation techniques, and key findings. Investigators examined a wide range of action anticipation scenarios specific to eight different sports and utilized functional magnetic resonance imaging (fMRI), electroencephalogram (EEG), and transcranial magnetic stimulation (TMS). Expert-novice comparisons were commonly used to investigate differences in action anticipation performance and neurophysiology. Experts tended to outperform novices, and an extensive array of brain structures were reported to be involved differently for experts and novices during action anticipation. However, these neurophysiological findings were generally inconsistent across the studies reviewed. The discussion focuses on strengths and four key limitations. The conclusion posits remaining questions and recommendations for future research.},
   author = {Daniel M. Smith},
   doi = {10.1016/j.neubiorev.2015.11.007},
   issn = {01497634},
   journal = {Neuroscience \& Biobehavioral Reviews},
   keywords = {Action anticipation,Athletes,Brain stimulation,FMRI,Neuroimaging},
   month = {1},
   pages = {115-120},
   pmid = {26616736},
   publisher = {Elsevier Ltd},
   title = {Neurophysiology of action anticipation in athletes: A systematic review},
   volume = {60},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0149763415302360},
   year = {2016},
}
@article{Castro2021,
   abstract = {<p>Repetitive industrial tasks can be easily performed by traditional robotic systems. However, many other works require cognitive knowledge that only humans can provide. Human-Robot Collaboration (HRC) emerges as an ideal concept of co-working between a human operator and a robot, representing one of the most significant subjects for human-life improvement.The ultimate goal is to achieve physical interaction, where handing over an object plays a crucial role for an effective task accomplishment. Considerable research work had been developed in this particular field in recent years, where several solutions were already proposed. Nonetheless, some particular issues regarding Human-Robot Collaboration still hold an open path to truly important research improvements. This paper provides a literature overview, defining the HRC concept, enumerating the distinct human-robot communication channels, and discussing the physical interaction that this collaboration entails. Moreover, future challenges for a natural and intuitive collaboration are exposed: the machine must behave like a human especially in the pre-grasping/grasping phases and the handover procedure should be fluent and bidirectional, for an articulated function development. These are the focus of the near future investigation aiming to shed light on the complex combination of predictive and reactive control mechanisms promoting coordination and understanding. Following recent progress in artificial intelligence, learning exploration stand as the key element to allow the generation of coordinated actions and their shaping by experience.</p>},
   author = {Afonso Castro and Filipe Silva and Vitor Santos},
   doi = {10.3390/s21124113},
   issn = {1424-8220},
   issue = {12},
   journal = {Sensors},
   keywords = {Human-Robot Collaboration,Interfaces of communication,Object handover,Physicality,Robot cognition},
   month = {6},
   pages = {4113},
   pmid = {34203766},
   publisher = {MDPI AG},
   title = {Trends of Human-Robot Collaboration in Industry Contexts: Handover, Learning, and Metrics},
   volume = {21},
   url = {https://www.mdpi.com/1424-8220/21/12/4113},
   year = {2021},
}
@article{Villani2018,
   abstract = {Easy-to-use collaborative robotics solutions, where human workers and robots share their skills, are entering the market, thus becoming the new frontier in industrial robotics. They allow to combine the advantages of robots, which enjoy high levels of accuracy, speed and repeatability, with the flexibility and cognitive skills of human workers. However, to achieve an efficient human–robot collaboration, several challenges need to be tackled. First, a safe interaction must be guaranteed to prevent harming humans having a direct contact with the moving robot. Additionally, to take full advantage of human skills, it is important that intuitive user interfaces are properly designed, so that human operators can easily program and interact with the robot. In this survey paper, an extensive review on human–robot collaboration in industrial environment is provided, with specific focus on issues related to physical and cognitive interaction. The commercially available solutions are also presented and the main industrial applications where collaborative robotic is advantageous are discussed, highlighting how collaborative solutions are intended to improve the efficiency of the system and which the open issue are.},
   author = {Valeria Villani and Fabio Pini and Francesco Leali and Cristian Secchi},
   doi = {10.1016/j.mechatronics.2018.02.009},
   issn = {09574158},
   journal = {Mechatronics},
   keywords = {Collaborative robots,Human–robot collaboration,Industrial applications,Intuitive robot programming,Safety,User interfaces},
   month = {11},
   pages = {248-266},
   publisher = {Elsevier Ltd},
   title = {Survey on human–robot collaboration in industrial settings: Safety, intuitive interfaces and applications},
   volume = {55},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0957415818300321},
   year = {2018},
}
@unpublished{Kreiss2021,
   abstract = {Many image-based perception tasks can be formulated as detecting, associating and tracking semantic keypoints, e.g., human body pose estimation and tracking. In this work, we present a general framework that jointly detects and forms spatio-temporal keypoint associations in a single stage, making this the first real-time pose detection and tracking algorithm. We present a generic neural network architecture that uses Composite Fields to detect and construct a spatio-temporal pose which is a single, connected graph whose nodes are the semantic keypoints (e.g., a person's body joints) in multiple frames. For the temporal associations, we introduce the Temporal Composite Association Field (TCAF) which requires an extended network architecture and training method beyond previous Composite Fields. Our experiments show competitive accuracy while being an order of magnitude faster on multiple publicly available datasets such as COCO, CrowdPose and the PoseTrack 2017 and 2018 datasets. We also show that our method generalizes to any class of semantic keypoints such as car and animal parts to provide a holistic perception framework that is well suited for urban mobility such as self-driving cars and delivery robots.},
   author = {Sven Kreiss and Lorenzo Bertoni and Alexandre Alahi},
   month = {3},
   title = {OpenPifPaf: Composite Fields for Semantic Keypoint Detection and Spatio-Temporal Association},
   url = {http://arxiv.org/abs/2103.02440},
   year = {2021},
   note = {unpublished}
}
@unpublished{Kreiss2019,
   abstract = {We propose a new bottom-up method for multi-person 2D human pose estimation that is particularly well suited for urban mobility such as self-driving cars and delivery robots. The new method, PifPaf, uses a Part Intensity Field (PIF) to localize body parts and a Part Association Field (PAF) to associate body parts with each other to form full human poses. Our method outperforms previous methods at low resolution and in crowded, cluttered and occluded scenes thanks to (i) our new composite field PAF encoding fine-grained information and (ii) the choice of Laplace loss for regressions which incorporates a notion of uncertainty. Our architecture is based on a fully convolutional, single-shot, box-free design. We perform on par with the existing state-of-the-art bottom-up method on the standard COCO keypoint task and produce state-of-the-art results on a modified COCO keypoint task for the transportation domain.},
   author = {Sven Kreiss and Lorenzo Bertoni and Alexandre Alahi},
   month = {3},
   title = {PifPaf: Composite Fields for Human Pose Estimation},
   url = {http://arxiv.org/abs/1903.06593},
   year = {2019},
   note = {unpublished},
}
@article{ROS2,
    author = {Steven Macenski  and Tully Foote  and Brian Gerkey  and Chris Lalancette  and William Woodall },
    title = {Robot Operating System 2: Design, architecture, and uses in the wild},
    journal = {Science Robotics},
    volume = {7},
    number = {66},
    pages = {eabm6074},
    year = {2022},
    doi = {10.1126/scirobotics.abm6074},
    URL = {https://www.science.org/doi/abs/10.1126/scirobotics.abm6074}
}
@manual{BoschSensorFlyer,
   organization = {Bosch Sensortec},
   title = {BHI260AP Flyer},
}
@manual{BoschSensorSetupGuide,
   organization = {Bosch Sensortec},
   title = {BHI260AB/BHA260AB Evaluation Setup Guide},
}
@manual{BoschDD2,
   organization = {Bosch Sensortec},
   title = {BHYxxx Desktop Development 2.0 User Manual},
}
@article{Sarker2021,
   abstract = {In the current age of the Fourth Industrial Revolution (4IR or Industry 4.0), the digital world has a wealth of data, such as Internet of Things (IoT) data, cybersecurity data, mobile data, business data, social media data, health data, etc. To intelligently analyze these data and develop the corresponding smart and automated applications, the knowledge of artificial intelligence (AI), particularly, machine learning (ML) is the key. Various types of machine learning algorithms such as supervised, unsupervised, semi-supervised, and reinforcement learning exist in the area. Besides, the deep learning, which is part of a broader family of machine learning methods, can intelligently analyze the data on a large scale. In this paper, we present a comprehensive view on these machine learning algorithms that can be applied to enhance the intelligence and the capabilities of an application. Thus, this study’s key contribution is explaining the principles of different machine learning techniques and their applicability in various real-world application domains, such as cybersecurity systems, smart cities, healthcare, e-commerce, agriculture, and many more. We also highlight the challenges and potential research directions based on our study. Overall, this paper aims to serve as a reference point for both academia and industry professionals as well as for decision-makers in various real-world situations and application areas, particularly from the technical point of view.},
   author = {Iqbal H. Sarker},
   doi = {10.1007/s42979-021-00592-x},
   issn = {2662-995X},
   issue = {3},
   journal = {SN Computer Science},
   keywords = {Artificial intelligence,Data science,Data-driven decision-making,Deep learning,Intelligent applications,Machine learning,Predictive analytics},
   month = {5},
   pages = {160},
   publisher = {Springer},
   title = {Machine Learning: Algorithms, Real-World Applications and Research Directions},
   volume = {2},
   url = {https://link.springer.com/10.1007/s42979-021-00592-x},
   year = {2021},
}
@misc{lstm_advantages,
   author = {Tiago Miguel},
   title = {How the LSTM improves the RNN},
   note = {Last accessed 20 January 2023},
   url = {https://towardsdatascience.com/how-the-lstm-improves-the-rnn-1ef156b75121},
   year = {2021},
}
@article{Alom2019,
   abstract = {<p>In recent years, deep learning has garnered tremendous success in a variety of application domains. This new field of machine learning has been growing rapidly and has been applied to most traditional application domains, as well as some new areas that present more opportunities. Different methods have been proposed based on different categories of learning, including supervised, semi-supervised, and un-supervised learning. Experimental results show state-of-the-art performance using deep learning when compared to traditional machine learning approaches in the fields of image processing, computer vision, speech recognition, machine translation, art, medical imaging, medical information processing, robotics and control, bioinformatics, natural language processing, cybersecurity, and many others. This survey presents a brief survey on the advances that have occurred in the area of Deep Learning (DL), starting with the Deep Neural Network (DNN). The survey goes on to cover Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). Additionally, we have discussed recent developments, such as advanced variant DL techniques based on these DL approaches. This work considers most of the papers published after 2012 from when the history of deep learning began. Furthermore, DL approaches that have been explored and evaluated in different application domains are also included in this survey. We also included recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys that have been published on DL using neural networks and a survey on Reinforcement Learning (RL). However, those papers have not discussed individual advanced techniques for training large-scale deep learning models and the recently developed method of generative models.</p>},
   author = {Md Zahangir Alom and Tarek M. Taha and Chris Yakopcic and Stefan Westberg and Paheding Sidike and Mst Shamima Nasrin and Mahmudul Hasan and Brian C. Van Essen and Abdul A. S. Awwal and Vijayan K. Asari},
   doi = {10.3390/electronics8030292},
   issn = {2079-9292},
   issue = {3},
   journal = {Electronics},
   keywords = {Auto-encoder (AE),Convolutional neural network (CNN),Deep belief network (DBN),Deep learning,Deep reinforcement learning (DRL),Generative adversarial network (GAN),Recurrent neural network (RNN),Restricted Boltzmann machine (RBM),Transfer learning},
   month = {3},
   pages = {292},
   publisher = {MDPI AG},
   title = {A State-of-the-Art Survey on Deep Learning Theory and Architectures},
   volume = {8},
   url = {https://www.mdpi.com/2079-9292/8/3/292},
   year = {2019},
}
@article{Li2023,
   abstract = {To facilitate the personalized smart manufacturing paradigm with cognitive automation capabilities, Deep Reinforcement Learning (DRL) has attracted ever-increasing attention by offering an adaptive and flexible solution. DRL takes the advantages of both Deep Neural Networks (DNN) and Reinforcement Learning (RL), by embracing the power of representation learning, to make precise and fast decisions when facing dynamic and complex situations. Ever since the first paper of DRL was published in 2013, its applications have sprung up across the manufacturing field with exponential publication growth year by year. However, there still lacks any comprehensive review of the DRL in the field of smart manufacturing. To fill this gap, a systematic review process was conducted, with 261 relevant publications selected to date (20-Oct-2022), to gain a holistic understanding of the development, application, and challenges of DRL in smart manufacturing along the whole engineering lifecycle. First, the concept and development of DRL are summarized. Then, the typical DRL applications are analyzed in the four engineering lifecycle stages: design, manufacturing, distribution, and maintenance. Finally, the challenges and future directions are illustrated, especially emerging DRL-related technologies and solutions that can improve the manufacturing system's deployment feasibility, cognitive capability, and learning efficiency, respectively. It is expected that this work can provide an insightful guide to the research of DRL in the smart manufacturing field and shed light on its future perspectives.},
   author = {Chengxi Li and Pai Zheng and Yue Yin and Baicun Wang and Lihui Wang},
   doi = {10.1016/j.cirpj.2022.11.003},
   issn = {17555817},
   journal = {CIRP Journal of Manufacturing Science and Technology},
   keywords = {Artificial intelligence,Deep reinforcement learning,Engineering life cycle,Review,Smart manufacturing},
   month = {2},
   pages = {75-101},
   publisher = {Elsevier Ltd},
   title = {Deep reinforcement learning in smart manufacturing: A review and prospects},
   volume = {40},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S1755581722001717},
   year = {2023},
}
@inproceedings{Huang2016,
   abstract = {Efficient collaboration requires collaborators to monitor the behaviors of their partners, make inferences about their task intent, and plan their own actions accordingly. To work seamlessly and efficiently with their human counterparts, robots must similarly rely on predictions of their users' intent in planning their actions. In this paper, we present an anticipatory control method that enables robots to proactively perform task actions based on anticipated actions of their human partners. We implemented this method into a robot system that monitored its user's gaze, predicted his or her task intent based on observed gaze patterns, and performed anticipatory task actions according to its predictions. Results from a human-robot interaction experiment showed that anticipatory control enabled the robot to respond to user requests and complete the task faster-2.5 seconds on average and up to 3.4 seconds-compared to a robot using a reactive control method that did not anticipate user intent. Our findings highlight the promise of performing anticipatory actions for achieving efficient human-robot teamwork.},
   author = {Chien-Ming Huang and Bilge Mutlu},
   doi = {10.1109/HRI.2016.7451737},
   isbn = {978-1-4673-8370-7},
   issn = {21672148},
   booktitle = {2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
   keywords = {Action observation,Anticipatory action,Gaze,Human-robot collaboration,Intent prediction},
   month = {3},
   pages = {83-90},
   publisher = {IEEE},
   title = {Anticipatory robot control for efficient human-robot collaboration},
   url = {http://ieeexplore.ieee.org/document/7451737/},
   year = {2016},
}
@inproceedings{Tortora2019,
   abstract = {"Part Number: CFP19835-ART."},
   author = {Stefano Tortora and Stefano Michieletto and Francesca Stival and Emanuele Menegatti},
   doi = {10.1109/CIS-RAM47153.2019.9095779},
   isbn = {978-1-7281-3458-1},
   booktitle = {2019 IEEE International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM)},
   month = {11},
   pages = {457-462},
   publisher = {IEEE},
   title = {Fast human motion prediction for human-robot collaboration with wearable interface},
   url = {https://ieeexplore.ieee.org/document/9095779/},
   year = {2019},
}
@misc{AstraPro,
   author = {Orbbec},
   title = {ASTRA PRO PLUS},
   note = {Last accessed 30 March 2023},
   url = {https://shop.orbbec3d.com/Astra-Pro-Plus},
}
@misc{UR10e_image,
   author = {WiredWorkers},
   title = {UR10e},
   note = {Last accessed 9 May 2023},
   url = {https://shop.wiredworkers.io/en_GB/shop/universal-robots-ur10e-87},
}
@misc{UR10e,
   author = {Universal Robots},
   title = {THE UR10e},
   note = {Last accessed 9 May 2023},
   url = {https://www.universal-robots.com/products/ur10-robot/},
}
@inproceedings{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. © 2017 Neural information processing systems foundation. All rights reserved.},
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and Łukasz Kaiser and Illia Polosukhin},
   editor = {Guyon I. and Fergus R. and Wallach H. and Wallach H. and Guyon I. and Vishwanathan S.V.N. and von Luxburg U. and Garnett R. and Vishwanathan S.V.N. and Bengio S. and Fergus R.},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   keywords = {Convolution; Decoding; Network architecture; Program processors; Signal encoding; Attention mechanisms; Best model; Bleu scores; Convolutional neural network; Single models; Training costs; Two machines; Recurrent neural networks},
   note = {Cited by: 31224; Conference name: 31st Annual Conference on Neural Information Processing Systems, NIPS 2017; Conference date: 4 December 2017 through 9 December 2017; Conference code: 136033},
   pages = {5999 – 6009},
   publisher = {Neural information processing systems foundation},
   title = {Attention is all you need},
   volume = {2017-December},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043317328&partnerID=40&md5=3e5a5c2b862c8979ffea845bb707b3c3},
   year = {2017},
}
