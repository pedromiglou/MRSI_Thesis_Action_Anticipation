@article{Liu2017,
title = "Human motion prediction for human-robot collaboration",
journal = "Journal of Manufacturing Systems",
volume = "44",
pages = "287 - 294",
year = "2017",
note = "Special Issue on Latest advancements in manufacturing systems at NAMRC 45",
issn = "0278-6125",
doi = "https://doi.org/10.1016/j.jmsy.2017.04.009",
url = "http://www.sciencedirect.com/science/article/pii/S0278612517300481",
author = "Hongyi Liu and Lihui Wang",
keywords = "Human-robot collaboration, Human motion prediction, Assembly",
}

@inproceedings{Kragic2018,
  title     = {Interactive, Collaborative Robots: Challenges and Opportunities},
  author    = {Danica Kragic and Joakim Gustafson and Hakan Karaoguz and Patric Jensfelt and Robert Krug},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on
              Artificial Intelligence, {IJCAI-18}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization}, 
  pages     = {18--25},
  year      = {2018},
  month     = {7},
   doi       = {10.24963/ijcai.2018/3},
  url       = {https://doi.org/10.24963/ijcai.2018/3},
}

@inproceedings{Chandrasekaran2015,  
author={B. {Chandrasekaran} and J. M. {Conrad}},  booktitle={SoutheastCon},   
title={Human-robot collaboration: A survey},   year={2015},
pages={1-8},  
doi={10.1109/SECON.2015.7132964}}

@ARTICLE{Hoffman2019,  
author={G. {Hoffman}},  
journal={IEEE Transactions on Human-Machine Systems},   title={Evaluating Fluency in Human–Robot Collaboration},   year={2019},  
volume={49},  
number={3},  
pages={209-218},  doi={ 10.1109/THMS.2019.2904558}}

@article{Ajoudani2018,
author = {Ajoudani, Arash and Zanchettin, Andrea Maria and Ivaldi, Serena and Albu-Schäffer, Alin and Kosuge, Kazuhiro and Khatib, Oussama},
year = {2018},
month = {06},
pages = {},
title = {Progress and Prospects of the Human-Robot Collaboration},
volume = {42},
journal = {Autonomous Robots},
doi = {10.1007/s10514-017-9677-2}
}

@article{Rozo2018,
author = {Rozo, Leonel and Ben Amor, Heni and Calinon, Sylvain and Dragan, Anca and Lee, Dongheui},
year = {2018},
month = {04},
pages = {},
title = {Special issue on learning for human–robot collaboration},
volume = {42},
journal = {Autonomous Robots},
doi = {10.1007/s10514-018-9756-z}
}

@misc{Kumar2021,
author = {Kumar, Shitij and Savur, Celal and Sahin, Ferat},
booktitle = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
doi = {10.1109/TSMC.2020.3041231},
issn = {21682232},
keywords = {Awareness,compliance,digital-twin human-robot collaboration (HRC),industrial automation,intelligence,physiological computing,speed and separation monitoring (SSM)},
title = {{Survey of Human-Robot Collaboration in Industrial Settings: Awareness, Intelligence, and Compliance}},
year = {2021}
}

@article{ElZaatari2019,
title = "Cobot programming for collaborative industrial tasks: An overview",
journal = "Robotics and Autonomous Systems",
volume = "116",
pages = "162 - 180",
year = "2019",
issn = "0921-8890",
doi = "https://doi.org/10.1016/j.robot.2019.03.003",
url = "http://www.sciencedirect.com/science/article/pii/S092188901830602X",
author = "Shirine {El Zaatari} and Mohamed Marei and Weidong Li and Zahid Usman",
}

@ARTICLE{RoblaGomez2017,
  author={S. {Robla-Gómez} and V. M. {Becerra} and J. R. {Llata} and E. {González-Sarabia} and C. {Torre-Ferrero} and J. {Pérez-Oria}},
  journal={IEEE Access}, 
  title={Working Together: A Review on Safe Human-Robot Collaboration in Industrial Environments}, 
  year={2017},
  volume={5},
  number={},
  pages={26754-26773},
  doi={10.1109/ACCESS.2017.2773127}}

@article{Matheson2019,
  title={Human-Robot Collaboration in Manufacturing Applications: A Review},
  author={E. Matheson and Riccardo Minto and Emanuele G. G. Zampieri and M. Faccio and G. Rosati},
  journal={Robotics},
  year={2019},
  volume={8},
  pages={100}
}

@article{Jiao2020,
  title={Towards augmenting cyber-physical-human collaborative cognition for human-automation interaction in complex manufacturing and operational environments},
  author={Jiao, Jianxin and Zhou, Feng and Gebraeel, Nagi Z and Duffy, Vincent},
  journal={International Journal of Production Research},
  volume={58},
  number={16},
  pages={5089--5111},
  year={2020},
  publisher={Taylor \& Francis}
}

@article{Michalos2018,
  title={Seamless human robot collaborative assembly--An automotive case study},
  author={Michalos, George and Kousi, Niki and Karagiannis, Panagiotis and Gkournelos, Christos and Dimoulas, Konstantinos and Koukas, Spyridon and Mparis, Konstantinos and Papavasileiou, Apostolis and Makris, Sotiris},
  journal={Mechatronics},
  volume={55},
  pages={194--211},
  year={2018},
  publisher={Elsevier}
}

@article{Papanastasiou2019,
  title={Towards seamless human robot collaboration: integrating multimodal interaction},
  author={Papanastasiou, Stergios and Kousi, Niki and Karagiannis, Panagiotis and Gkournelos, Christos and Papavasileiou, Apostolis and Dimoulas, Konstantinos and Baris, Konstantinos and Koukas, Spyridon and Michalos, George and Makris, Sotiris},
  journal={The International Journal of Advanced Manufacturing Technology},
  volume={105},
  pages={3881--3897},
  year={2019},
  publisher={Springer}
}

@article{Duarte2018,
  title={Action anticipation: Reading the intentions of humans and robots},
  author={Duarte, Nuno Ferreira and Rakovi{\'c}, Mirko and Tasevski, Jovica and Coco, Moreno Ignazio and Billard, Aude and Santos-Victor, Jos{\'e}},
  journal={IEEE Robotics and Automation Letters},
  volume={3},
  number={4},
  pages={4132--4139},
  year={2018},
  publisher={IEEE}
}

@article{Williams2009,
  title={Perceiving the intentions of others: how do skilled performers make anticipation judgments?},
  author={Williams, A Mark},
  journal={Progress in brain research},
  volume={174},
  pages={73--83},
  year={2009},
  publisher={Elsevier}
}

@article{Huang2015,
  title={Using gaze patterns to predict task intent in collaboration},
  author={Huang, Chien-Ming and Andrist, Sean and Saupp{\'e}, Allison and Mutlu, Bilge},
  journal={Frontiers in psychology},
  volume={6},
  pages={1049},
  year={2015},
  publisher={Frontiers Media SA}
}

@inproceedings{Gorur2018,
  title={Social cobots: Anticipatory decision-making for collaborative robots incorporating unexpected human behaviors},
  author={G{\"o}r{\"u}r, O Can and Rosman, Benjamin and Sivrikaya, Fikret and Albayrak, Sahin},
  booktitle={Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
  pages={398--406},
  year={2018}
}

@inproceedings{Gkioxari2018,
  title={Detecting and recognizing human-object interactions},
  author={Gkioxari, Georgia and Girshick, Ross and Doll{\'a}r, Piotr and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8359--8367},
  year={2018}
}

@article{Gupta2015,
  title={Visual semantic role labeling},
  author={Gupta, Saurabh and Malik, Jitendra},
  journal={arXiv preprint arXiv:1505.04474},
  year={2015}
}

@inproceedings{Chao2018,
  title={Learning to detect human-object interactions},
  author={Chao, Yu-Wei and Liu, Yunfan and Liu, Xieyang and Zeng, Huayi and Deng, Jia},
  booktitle={2018 ieee winter conference on applications of computer vision (wacv)},
  pages={381--389},
  year={2018},
  organization={IEEE}
}

@inproceedings{Zhuang2018,
  title={HCVRD: A benchmark for large-scale human-centered visual relationship detection},
  author={Zhuang, Bohan and Wu, Qi and Shen, Chunhua and Reid, Ian and van den Hengel, Anton},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  year={2018}
}


@inproceedings{Liu2021,
  title={Semi-supervised 3d hand-object poses estimation with interactions in time},
  author={Liu, Shaowei and Jiang, Hanwen and Xu, Jiarui and Liu, Sifei and Wang, Xiaolong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14687--14697},
  year={2021}
}


@article{Fan2018,
  title={What is that in your hand? recognizing grasped objects via forearm electromyography sensing},
  author={Fan, Junjun and Fan, Xiangmin and Tian, Feng and Li, Yang and Liu, Zitao and Sun, Wei and Wang, Hongan},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume={2},
  number={4},
  pages={1--24},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{Feix2014,
  title={Analysis of human grasping behavior: Object characteristics and grasp type},
  author={Feix, Thomas and Bullock, Ian M and Dollar, Aaron M},
  journal={IEEE transactions on haptics},
  volume={7},
  number={3},
  pages={311--323},
  year={2014},
  publisher={IEEE}
}

@inproceedings{Valkov2023,
  title={Reach Prediction using Finger Motion Dynamics},
  author={Valkov, Dimitar and Kockwelp, Pascal and Daiber, Florian and Kr{\"u}ger, Antonio},
  booktitle={Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--8},
  year={2023}
}

@article{Paulson2011,
  title={Object interaction detection using hand posture cues in an office setting},
  author={Paulson, Brandon and Cummings, Danielle and Hammond, Tracy},
  journal={International journal of human-computer studies},
  volume={69},
  number={1-2},
  pages={19--29},
  year={2011},
  publisher={Elsevier}
}

@article{Vatavu2013,
  title={Automatic recognition of object size and shape via user-dependent measurements of the grasping hand},
  author={Vatavu, Radu-Daniel and Zai{\c{t}}i, Ionu{\c{t}} Alexandru},
  journal={International Journal of Human-Computer Studies},
  volume={71},
  number={5},
  pages={590--607},
  year={2013},
  publisher={Elsevier}
}

@article{Kuutti1996,
  title={Activity theory as a potential framework for human-computer interaction research},
  author={Kuutti, Kari and others},
  journal={Context and consciousness: Activity theory and human-computer interaction},
  volume={1744},
  pages={9--22},
  year={1996}
}

@book{Mackenzie1994,
  title={The grasping hand},
  author={MacKenzie, Christine L and Iberall, Thea},
  year={1994},
  publisher={Elsevier}
}

@article{Betti2018,
  title={Reach-to-grasp movements: a multimodal techniques study},
  author={Betti, Sonia and Zani, Giovanni and Guerra, Silvia and Castiello, Umberto and Sartori, Luisa},
  journal={Frontiers in psychology},
  volume={9},
  pages={990},
  year={2018},
  publisher={Frontiers Media SA}
}

@article{Egmose2018,
  title={Shaping of reach-to-grasp kinematics by intentions: A meta-analysis},
  author={Egmose, Ida and K{\o}ppe, Simo},
  journal={Journal of Motor Behavior},
  volume={50},
  number={2},
  pages={155--165},
  year={2018},
  publisher={Taylor \& Francis}
}

@inproceedings{Furnari2019,
  title={What would you expect? anticipating egocentric actions with rolling-unrolling lstms and modality attention},
  author={Furnari, Antonino and Farinella, Giovanni Maria},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6252--6261},
  year={2019}
}

@inproceedings{Hayes2017,
  title={Interpretable models for fast activity recognition and anomaly explanation during collaborative robotics tasks},
  author={Hayes, Bradley and Shah, Julie A},
  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={6586--6593},
  year={2017},
  organization={IEEE}
}

@article{Roy2021,
  title={Action anticipation using pairwise human-object interactions and transformers},
  author={Roy, Debaditya and Fernando, Basura},
  journal={IEEE Transactions on Image Processing},
  volume={30},
  pages={8116--8129},
  year={2021},
  publisher={IEEE}
}

@article{Xu2019,
  title={Interact as you intend: Intention-driven human-object interaction detection},
  author={Xu, Bingjie and Li, Junnan and Wong, Yongkang and Zhao, Qi and Kankanhalli, Mohan S},
  journal={IEEE Transactions on Multimedia},
  volume={22},
  number={6},
  pages={1423--1432},
  year={2019},
  publisher={IEEE}
}

@article{Koppula2015,
  title={Anticipating human activities using object affordances for reactive robotic response},
  author={Koppula, Hema S and Saxena, Ashutosh},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={38},
  number={1},
  pages={14--29},
  year={2015},
  publisher={IEEE}
}

@inproceedings{Quigley2009,
  title={ROS: an open-source Robot Operating System},
  author={Quigley, Morgan and Conley, Ken and Gerkey, Brian and Faust, Josh and Foote, Tully and Leibs, Jeremy and Wheeler, Rob and Ng, Andrew Y and others},
  booktitle={ICRA workshop on open source software},
  volume={3},
  pages={5},
  year={2009},
  organization={Kobe, Japan}
}

@article{Latreche2023,
  title={Reliability and validity analysis of MediaPipe-based measurement system for some human rehabilitation motions},
  author={Latreche, Ameur and Kelaiaia, Ridha and Chemori, Ahmed and Kerboua, Adlen},
  journal={Measurement},
  volume={214},
  pages={112826},
  year={2023},
  publisher={Elsevier}
}

@article{Amprimo2023,
  title={Hand tracking for clinical applications: validation of the Google MediaPipe Hand (GMH) and the depth-enhanced GMH-D frameworks},
  author={Amprimo, Gianluca and Masi, Giulia and Pettiti, Giuseppe and Olmo, Gabriella and Priano, Lorenzo and Ferraris, Claudia},
  journal={arXiv preprint arXiv:2308.01088},
  year={2023}
}

@inproceedings{Lugaresi2019,
  title={Mediapipe: A framework for perceiving and processing reality},
  author={Lugaresi, Camillo and Tang, Jiuqiang and Nash, Hadon and McClanahan, Chris and Uboweja, Esha and Hays, Michael and Zhang, Fan and Chang, Chuo-Ling and Yong, Ming and Lee, Juhyun and others},
  booktitle={Third workshop on computer vision for AR/VR at IEEE computer vision and pattern recognition (CVPR)},
  volume={2019},
  year={2019}
}

@article{Zhang2020,
  title={Mediapipe hands: On-device real-time hand tracking},
  author={Zhang, Fan and Bazarevsky, Valentin and Vakunov, Andrey and Tkachenka, Andrei and Sung, George and Chang, Chuo-Ling and Grundmann, Matthias},
  journal={arXiv preprint arXiv:2006.10214},
  year={2020}
}

@inproceedings{Amprimo2022,
  title={Gmh-d: Combining google mediapipe and rgb-depth cameras for hand motor skills remote assessment},
  author={Amprimo, Gianluca and Ferraris, Claudia and Masi, Giulia and Pettiti, Giuseppe and Priano, Lorenzo},
  booktitle={2022 IEEE International Conference on Digital Health (ICDH)},
  pages={132--141},
  year={2022},
  organization={IEEE}
}


@inbook{Taubin1992,
    author = {Taubin, Gabriel and Cooper, David B.},
    title = {Object Recognition Based on Moment (or Algebraic) Invariants},
    year = {1992},
    isbn = {0262132850},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    booktitle = {Geometric Invariance in Computer Vision},
    pages = {375–397},
    numpages = {23}
}

@InProceedings{Mindru1999,
doi="10.1007/978-1-4471-0833-7_12",
author="Mindru, Florica and Moons, Theo and Van Gool, Luc",
editor="Singh, Sameer",
title="Color-Based Moment Invariants for Viewpoint and Illumination Independent Recognition of Planar Color Patterns",
booktitle="International Conference on Advances in Pattern Recognition",
year="1999",
publisher="Springer London",
address="London",
pages="113--122",
abstract="This paper contributes to the viewpoint and illumination independent recognition of planar color patterns such as labels, logos, signs, pictograms, etc. by means of moment invariants. It introduces the idea of using powers of the intensities in the different color bands of a color image and combinations thereof for the construction of the moments. First, a complete classification is made of all functions of such moments which are invariant under both affine deformations of the pattern (thus achieving viewpoint invariance) as well as linear changes of the intensity values in the individual color bands (hence, coping with changes in the irradiance pattern due to different lighting conditions and/or viewpoints). The discriminant power and classification performance of these new invariants for color pattern recogniti on has been tested on a data set consisting of images of real outdoors advertising panels. Furthermore, a comparison to moment invariants presented in literature ([1] and [2]) that come closest sto the aimed type of invariants is made and new approaches to improve their performance are presented.",
isbn="978-1-4471-0833-7"
}

@INPROCEEDINGS{Sarfraz2006,
  author={Sarfraz, M.},
  booktitle={Geometric Modeling and Imaging--New Trends (GMAI'06)}, 
  title={Object Recognition Using Moments: Some Experiments and Observations}, 
  year={2006},
  volume={},
  number={},
  pages={189-194},
  doi={10.1109/GMAI.2006.39}
}


@article{Wu2020,
title = {Recent advances in deep learning for object detection},
journal = {Neurocomputing},
volume = {396},
pages = {39-64},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.01.085},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220301430},
author = {Xiongwei Wu and Doyen Sahoo and Steven C.H. Hoi},
keywords = {Object detection, Deep learning, Deep convolutional neural networks},
abstract = {Object detection is a fundamental visual recognition problem in computer vision and has been widely studied in the past decades. Visual object detection aims to find objects of certain target classes with precise localization in a given image and assign each object instance a corresponding class label. Due to the tremendous successes of deep learning based image classification, object detection techniques using deep learning have been actively studied in recent years. In this paper, we give a comprehensive survey of recent advances in visual object detection with deep learning. By reviewing a large body of recent related work in literature, we systematically analyze the existing object detection frameworks and organize the survey into three major parts: (i) detection components, (ii) learning strategies, and (iii) applications \& benchmarks. In the survey, we cover a variety of factors affecting the detection performance in detail, such as detector architectures, feature learning, proposal generation, sampling strategies, etc. Finally, we discuss several future directions to facilitate and spur future research for visual object detection with deep learning.}
}


@conference{Barabanau2020,
author={Ivan Barabanau and Alexey Artemov and Evgeny Burnaev and Vyacheslav Murashkin},
title={Monocular 3D Object Detection via Geometric Reasoning on Keypoints},
booktitle={Proceedings of the 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP 2020) - Volume 5: VISAPP},
year={2020},
pages={652-659},
publisher={SciTePress},
organization={INSTICC},
doi={10.5220/0009102506520659},
isbn={978-989-758-402-2},
issn={2184-4321}
}

@ARTICLE {Ren2017,
author = {S. Ren and K. He and R. Girshick and J. Sun},
journal = {IEEE Transactions on Pattern Analysis \& Machine Intelligence},
title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
year = {2017},
volume = {39},
number = {06},
issn = {1939-3539},
pages = {1137-1149},
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with attention mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
keywords = {proposals;object detection;convolutional codes;feature extraction;search problems;detectors;training},
doi = {10.1109/TPAMI.2016.2577031},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {06}
}

@ARTICLE{Zhuang2021,
  author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal={Proceedings of the IEEE}, 
  title={A Comprehensive Survey on Transfer Learning}, 
  year={2021},
  volume={109},
  number={1},
  pages={43-76},
  doi={10.1109/JPROC.2020.3004555}
}


@article{Rato2022,
title = {A sensor-to-pattern calibration framework for multi-modal industrial collaborative cells},
journal = {Journal of Manufacturing Systems},
volume = {64},
pages = {497-507},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001182},
author = {Daniela Rato and Miguel Oliveira and Vítor Santos and Manuel Gomes and Angel Sappa},
keywords = {Calibration, Collaborative cell, Multi-modal, Multi-sensor},
abstract = {Collaborative robotic industrial cells are workspaces where robots collaborate with human operators. In this context, safety is paramount, and for that a complete perception of the space where the collaborative robot is inserted is necessary. To ensure this, collaborative cells are equipped with a large set of sensors of multiple modalities, covering the entire work volume. However, the fusion of information from all these sensors requires an accurate extrinsic calibration. The calibration of such complex systems is challenging, due to the number of sensors and modalities, and also due to the small overlapping fields of view between the sensors, which are positioned to capture different viewpoints of the cell. This paper proposes a sensor to pattern methodology that can calibrate a complex system such as a collaborative cell in a single optimization procedure. Our methodology can tackle RGB and Depth cameras, as well as LiDARs. Results show that our methodology is able to accurately calibrate a collaborative cell containing three RGB cameras, a depth camera and three 3D LiDARs.}
}

@inproceedings{Zimmermann2018,
author = {Zimmermann, Christian and Welschehold, Tim and Dornhege, Christian and Burgard, Wolfram and Brox, Thomas},
title = {3D Human Pose Estimation in RGBD Images for Robotic Task Learning},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICRA.2018.8462833},
doi = {10.1109/ICRA.2018.8462833},
abstract = {We propose an approach to estimate 3D human pose in real world units from a single RGBD image and show that it exceeds performance of monocular 3D pose estimation approaches from color as well as pose estimation exclusively from depth. Our approach builds on robust human keypoint detectors for color images and incorporates depth for lifting into 3D. We combine the system with our learning from demonstration framework to instruct a service robot without the need of markers. Experiments in real world settings demonstrate that our approach enables a PR2 robot to imitate manipulation actions observed from a human teacher.},
booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
pages = {1986–1992},
numpages = {7},
location = {Brisbane, Australia}
}

@article{Qi2021,
title = {Review of multi-view 3D object recognition methods based on deep learning},
journal = {Displays},
volume = {69},
pages = {102053},
year = {2021},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2021.102053},
url = {https://www.sciencedirect.com/science/article/pii/S0141938221000639},
author = {Shaohua Qi and Xin Ning and Guowei Yang and Liping Zhang and Peng Long and Weiwei Cai and Weijun Li},
keywords = {Multi-view, Deep Learning, 3D Object Classification, 3D Object Retrieval},
abstract = {Three-dimensional (3D) object recognition is widely used in automated driving, medical image analysis, virtual/augmented reality, artificial intelligence robots, and other areas. Deep learning is increasingly being used to solve 3D vision problems. Multi-view 3D object recognition based on the deep learning technique has become one of the rigorously researched topics because it can directly use the pretrained and successful advanced classification network as the backbone network, and views from multiple viewpoints can complement each other’s detailed features of the object. However, some challenges still exist in this area. Recently, many methods have been proposed to solve the problems pertaining to this research topic. This paper presents a comprehensive review and classification of the latest developments in the deep learning methods for multi-view 3D object recognition. It also summarizes the results of these methods on a few mainstream datasets, provides an insightful summary, and puts forward enlightening future research directions.}
}