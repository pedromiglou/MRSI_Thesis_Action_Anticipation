{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 14:28:31.469791: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-03 14:28:32.546777: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/flags/flags.csv\", header=None)\n",
    "\n",
    "flags = set([(df[1][i],df[2][i],df[3][i]) for i in range(0,14)])\n",
    "\n",
    "colors = [\"red\",\"lightblue\",\"blue\",\"green\",\"orange\",\"yellow\",\"white\"]\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for c1 in colors:\n",
    "    x.append(c1)\n",
    "    if c1 in [x[0] for x in flags]:\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)\n",
    "    \n",
    "    for c2 in colors:\n",
    "        x.append(c1+\",\"+c2)\n",
    "        if c2 in [x[1] for x in flags if x[0]==c1]:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "        \n",
    "        for c3 in colors:\n",
    "            x.append(c1+\",\"+c2+\",\"+c3)\n",
    "            if c3 in [x[2] for x in flags if x[0]==c1 and x[1]==c2]:\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)\n",
    "\n",
    "#y = np.array(df[0])\n",
    "\n",
    "#x = np.array([df[1][i]+\",\"+df[2][i]+\",\"+df[3][i] for i in range(0,len(y))])\n",
    "\n",
    "#x = [df[1][i]+\",\"+df[2][i] for i in range(0,14)]\n",
    "\n",
    "#y = list(df[3])\n",
    "\n",
    "x = np.array(x)\n",
    "\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " ...\n",
      " [7 7 5]\n",
      " [7 7 6]\n",
      " [7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20, split=\",\")\n",
    "\n",
    "# updates internal vocabulary based on the lyrics\n",
    "tokenizer.fit_on_texts(x)\n",
    "\n",
    "# transform each text in x to a sequence of tokens\n",
    "x = tokenizer.texts_to_sequences(x)\n",
    "\n",
    "x = np.array(pad_sequences(x, maxlen = 3))\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = len(y)\n",
    "\n",
    "#idx = np.random.permutation(len(x))\n",
    "#print(idx)\n",
    "#x = x[idx]\n",
    "#y = y[idx]\n",
    "\n",
    "#x_train, x_test = x[:int(num*0.8)], x[int(num*0.8):]\n",
    "#y_train, y_test = y[:int(num*0.8)], y[int(num*0.8):]\n",
    "\n",
    "x = x.reshape((x.shape[0], x.shape[1], 1))\n",
    "#x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "#x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 14:28:35.810625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 14:28:35.856983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 14:28:35.857713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 14:28:35.859668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 14:28:35.860984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 14:28:35.861520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 14:28:36.807275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 14:28:36.807664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 14:28:36.808179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-03 14:28:36.808539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3347 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 3, 1)        2           ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 3, 1)        7169        ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 3, 1)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 3, 1)        0           ['dropout[0][0]',                \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 3, 1)        2           ['tf.__operators__.add[0][0]']   \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 3, 4)         8           ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 3, 4)         0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 3, 1)         5           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 3, 1)        0           ['conv1d_1[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 3, 1)        2           ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 3, 1)        7169        ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 3, 1)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 3, 1)        0           ['dropout_2[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 3, 1)        2           ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 3, 4)         8           ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 3, 4)         0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 3, 1)         5           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 3, 1)        0           ['conv1d_3[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 3, 1)        2           ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 3, 1)        7169        ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 3, 1)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 3, 1)        0           ['dropout_4[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 3, 1)        2           ['tf.__operators__.add_4[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 3, 4)         8           ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 3, 4)         0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 3, 1)         5           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 3, 1)        0           ['conv1d_5[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 3, 1)        2           ['tf.__operators__.add_5[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 3, 1)        7169        ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 3, 1)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 3, 1)        0           ['dropout_6[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 3, 1)        2           ['tf.__operators__.add_6[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 3, 4)         8           ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 3, 4)         0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 3, 1)         5           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 3, 1)        0           ['conv1d_7[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 3)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          512         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,385\n",
      "Trainable params: 29,385\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 14:28:46.668084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-03 14:28:47.530648: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x1d1af650 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-03 14:28:47.530695: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 960M, Compute Capability 5.0\n",
      "2023-05-03 14:28:47.537739: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-03 14:28:47.745301: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 21s 104ms/step - loss: 0.3176 - accuracy: 0.9248 - val_loss: 0.2384 - val_accuracy: 0.9248\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.2708 - accuracy: 0.9248 - val_loss: 0.2429 - val_accuracy: 0.9248\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.2510 - accuracy: 0.9248 - val_loss: 0.2322 - val_accuracy: 0.9248\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.2277 - accuracy: 0.9248 - val_loss: 0.2194 - val_accuracy: 0.9248\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.2165 - accuracy: 0.9248 - val_loss: 0.2060 - val_accuracy: 0.9248\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.2172 - accuracy: 0.9248 - val_loss: 0.2032 - val_accuracy: 0.9323\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.2037 - accuracy: 0.9348 - val_loss: 0.2031 - val_accuracy: 0.9248\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.2065 - accuracy: 0.9248 - val_loss: 0.2029 - val_accuracy: 0.9248\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.2093 - accuracy: 0.9223 - val_loss: 0.1927 - val_accuracy: 0.9373\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.1970 - accuracy: 0.9223 - val_loss: 0.1892 - val_accuracy: 0.9323\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.1911 - accuracy: 0.9273 - val_loss: 0.1857 - val_accuracy: 0.9298\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 34ms/step - loss: 0.1905 - accuracy: 0.9348 - val_loss: 0.1818 - val_accuracy: 0.9298\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.1904 - accuracy: 0.9273 - val_loss: 0.1882 - val_accuracy: 0.9248\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.1905 - accuracy: 0.9398 - val_loss: 0.1830 - val_accuracy: 0.9223\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.1974 - accuracy: 0.9348 - val_loss: 0.1774 - val_accuracy: 0.9348\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.2278 - accuracy: 0.9273 - val_loss: 0.1839 - val_accuracy: 0.9223\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.2139 - accuracy: 0.9248 - val_loss: 0.1875 - val_accuracy: 0.9373\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.1981 - accuracy: 0.9323 - val_loss: 0.1906 - val_accuracy: 0.9424\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.1913 - accuracy: 0.9273 - val_loss: 0.1756 - val_accuracy: 0.9398\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.1838 - accuracy: 0.9298 - val_loss: 0.1742 - val_accuracy: 0.9348\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.1861 - accuracy: 0.9348 - val_loss: 0.1792 - val_accuracy: 0.9248\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 35ms/step - loss: 0.2025 - accuracy: 0.9398 - val_loss: 0.1698 - val_accuracy: 0.9373\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.1858 - accuracy: 0.9373 - val_loss: 0.1768 - val_accuracy: 0.9323\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.1865 - accuracy: 0.9323 - val_loss: 0.1692 - val_accuracy: 0.9373\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.1718 - accuracy: 0.9424 - val_loss: 0.1684 - val_accuracy: 0.9348\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.1770 - accuracy: 0.9373 - val_loss: 0.1597 - val_accuracy: 0.9424\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.2011 - accuracy: 0.9173 - val_loss: 0.1818 - val_accuracy: 0.9298\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.1963 - accuracy: 0.9373 - val_loss: 0.1678 - val_accuracy: 0.9449\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.1782 - accuracy: 0.9449 - val_loss: 0.1605 - val_accuracy: 0.9424\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 34ms/step - loss: 0.1801 - accuracy: 0.9298 - val_loss: 0.1584 - val_accuracy: 0.9424\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 34ms/step - loss: 0.1963 - accuracy: 0.9398 - val_loss: 0.1584 - val_accuracy: 0.9424\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.2448 - accuracy: 0.9248 - val_loss: 0.2423 - val_accuracy: 0.9248\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.2312 - accuracy: 0.9248 - val_loss: 0.2470 - val_accuracy: 0.9248\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.2211 - accuracy: 0.9223 - val_loss: 0.2604 - val_accuracy: 0.9248\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.2304 - accuracy: 0.9248 - val_loss: 0.2638 - val_accuracy: 0.9248\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.2384 - accuracy: 0.9348 - val_loss: 0.1975 - val_accuracy: 0.9298\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.1920 - accuracy: 0.9348 - val_loss: 0.1988 - val_accuracy: 0.9273\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.1912 - accuracy: 0.9223 - val_loss: 0.2080 - val_accuracy: 0.9348\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.2595 - accuracy: 0.9148 - val_loss: 0.2575 - val_accuracy: 0.9248\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.2553 - accuracy: 0.9148 - val_loss: 0.2251 - val_accuracy: 0.9273\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 39ms/step - loss: 0.2191 - accuracy: 0.9373 - val_loss: 0.2018 - val_accuracy: 0.9298\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1584 - accuracy: 0.9424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15838107466697693, 0.9423558712005615]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = x.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0,\n",
    "    dropout=0,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    validation_data=(x,y),\n",
    "    #validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model.evaluate(x, y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./flags_validator_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./flags_validator_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./flags_validator_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
