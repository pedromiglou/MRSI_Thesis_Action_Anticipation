@inproceedings{Gammulle2019,
abstract = {Inspired by human neurological structures for action anticipation, we present an action anticipation model that enables the prediction of plausible future actions by forecasting both the visual and temporal future. In contrast to current state-of-the-art methods which first learn a model to predict future video features and then perform action anticipation using these features, the proposed framework jointly learns to perform the two tasks, future visual and temporal representation synthesis, and early action anticipation. The joint learning framework ensures that the predicted future embeddings are informative to the action anticipation task. Furthermore, through extensive experimental evaluations we demonstrate the utility of using both visual and temporal semantics of the scene, and illustrate how this representation synthesis could be achieved through a recurrent Generative Adversarial Network (GAN) framework. Our model outperforms the current state-of-the-art methods on multiple datasets: UCF101, UCF101-24, UT-Interaction and TV Human Interaction.},
archivePrefix = {arXiv},
arxivId = {1912.07148},
author = {Gammulle, Harshala and Denman, Simon and Sridharan, Sridha and Fookes, Clinton},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2019.00566},
eprint = {1912.07148},
isbn = {9781728148038},
issn = {15505499},
title = {{Predicting the future: A jointly learnt model for action anticipation}},
year = {2019}
}
@article{Wu2021,
abstract = {Anticipating actions before they are executed is crucial for a wide range of practical applications, including autonomous driving and robotics. In this paper, we study the egocentric action anticipation task, which predicts future action seconds before it is performed for egocentric videos. Previous approaches focus on summarizing the observed content and directly predicting future action based on past observations. We believe it would benefit the action anticipation if we could mine some cues to compensate for the missing information of the unobserved frames. We then propose to decompose the action anticipation into a series of future feature predictions. We imagine how the visual feature changes in the near future and then predicts future action labels based on these imagined representations. Differently, our ImagineRNN is optimized in a contrastive learning way instead of feature regression. We utilize a proxy task to train the ImagineRNN, i.e., selecting the correct future states from distractors. We further improve ImagineRNN by residual anticipation, i.e., changing its target to predicting the feature difference of adjacent frames instead of the frame content. This promotes the network to focus on our target, i.e., the future action, as the difference between adjacent frame features is more important for forecasting the future. Extensive experiments on two large-scale egocentric action datasets validate the effectiveness of our method. Our method significantly outperforms previous methods on both the seen test set and the unseen test set of the EPIC Kitchens Action Anticipation Challenge.},
archivePrefix = {arXiv},
arxivId = {2101.04924},
author = {Wu, Yu and Zhu, Linchao and Wang, Xiaohan and Yang, Yi and Wu, Fei},
doi = {10.1109/TIP.2020.3040521},
eprint = {2101.04924},
issn = {19410042},
journal = {IEEE Transactions on Image Processing},
keywords = {Action anticipation,action prediction,egocentric videos},
pmid = {33270562},
title = {{Learning to Anticipate Egocentric Actions by Imagination}},
year = {2021}
}
@inproceedings{Hoffman2015,
abstract = {Within the next few years, personal robots are expected to enter our homes, offices, schools, hospitals, construction sites, and workshops. For these robots to play a successful role in people's professional and personal lives, they need to display the kind of efficient and satisfying interaction that humans are accustomed to from each other. Designing this human-robot interaction is a multifaceted challenge, balancing requirements of the robot's appearance and behavior. A robot's appearance evokes interaction affordances and triggers emotional responses; its behavior communicates internal states, and can support action coordination and joint planning. Good HRI design should enlist both facets to enable untrained humans to work fluently and intuitively with the robot.In this talk I will present the approach we have been using in the past decade to develop several non-anthropomorphic robotic systems. The underlying principles of both appearance and behavioral design are movement, timing, and embodiment, acknowledging that human perception is highly sensitive to spatial cues, physical movement, and visual affordances.We design our robots' appearance using techniques from 3D animation, sculpture, industrial, and interaction design. Gestures and behaviors drive decisions on the robot's appearance and mechanical design. Starting from freehand sketches, the robot's personality is built as a computer animated character, setting the parameters and limits of the robot's degrees of freedom. Then, material and form studies are combined with functional requirements to settle on the final system design. I will exemplify this process on the design of several robots.On the behavioral side, we design around the notion of human-robot fluency—the ability to accurately mesh the robot's activity with that of a human partner. I present computational architectures rooted in timing, joint action, and embodied cognition. Specifically, I discuss anticipatory action for collaboration, and a model of priming through perceptual simulation. Both systems have been shown to have significant effects on the fluency of a human-robot team, and on humans' perception of the robot's intelligence, commitment, and even gender. I then describe an interactive robotic improvisation system that uses embodied gestures for simultaneous, yet responsive, joint musicianship.},
author = {Hoffman, Guy},
doi = {10.1145/2814940.2815016},
title = {{Designing Fluent Human-Robot Collaboration}},
year = {2015}
}
@inproceedings{Saponaro2013,
abstract = {In this paper, we propose a method to recognize human body movements and we combine it with the contextual knowledge of human-robot collaboration scenarios provided by an object affordances framework that associates actions with its effects and the objects involved in them. The aim is to equip humanoid robots with action prediction capabilities, allowing them to anticipate effects as soon as a human partner starts performing a physical action, thus enabling interactions between man and robot to be fast and natural. We consider simple actions that characterize a human-robot collaboration scenario with objects being manipulated on a table: inspired from automatic speech recognition techniques, we train a statistical gesture model in order to recognize those physical gestures in real time. Analogies and differences between the two domains are discussed, highlighting the requirements of an automatic gesture recognizer for robots in order to perform robustly and in real time. {\textcopyright} 2013 IEEE.},
author = {Saponaro, Giovanni and Salvi, Giampiero and Bernardino, Alexandre},
booktitle = {Proceedings of the 2013 International Conference on Collaboration Technologies and Systems, CTS 2013},
doi = {10.1109/CTS.2013.6567232},
isbn = {9781467364027},
title = {{Robot anticipation of human intentions through continuous gesture recognition}},
year = {2013}
}
@inproceedings{Rodriguez2019,
abstract = {Human action-anticipation methods predict what is the future action by observing only a few portion of an action in progress. This is critical for applications where computers have to react to human actions as early as possible such as autonomous driving, human-robotic interaction, assistive robotics among others. In this paper, we present a method for human action anticipation by predicting the most plausible future human motion. We represent human motion using Dynamic Images [1] and make use of tailored loss functions to encourage a generative model to produce accurate future motion prediction. Our method outperforms the currently best performing action-anticipation methods by 4{\%} on JHMDB-21, 5.2{\%} on UT-Interaction and 5.1{\%} on UCF 101-24 benchmarks.},
archivePrefix = {arXiv},
arxivId = {1808.00141},
author = {Rodriguez, Cristian and Fernando, Basura and Li, Hongdong},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-11015-4_10},
eprint = {1808.00141},
isbn = {9783030110147},
issn = {16113349},
keywords = {Action-anticipation,Dynamic image,Generation,Motion representation,Prediction},
title = {{Action anticipation by predicting future dynamic images}},
year = {2019}
}
@article{Ivaldi2017,
abstract = {Future robots will need more and more anticipation capabilities, to properly react to human actions and provide efficient collaboration. To achieve this goal, we need new technologies that not only estimate the motion of the humans, but that fully describe the whole-body dynamics of the interaction and that can also predict its outcome. These hardware and software technologies are the goal of the European project AnDy. In this paper, we describe the roadmap of AnDy, which leverages existing technologies to endow robots with the ability to control physical collaboration through intentional interaction. To achieve this goal, AnDy relies on three technological and scientific breakthroughs. First, AnDy will innovate the way of measuring human whole-body motions by developing the wearable AnDySuit, which tracks motions and records forces. Second, AnDy will develop the AnDyModel, which combines ergonomic models with cognitive predictive models of human dynamic behavior in collaborative tasks, learned from data acquired with the AnDySuit. Third, AnDy will propose AnDyControl, an innovative technology for assisting humans through pre-dictive physical control, based on AnDyModel. By measuring and modeling human whole-body dynamics, AnDy will provide robots with a new level of awareness about human intentions and ergonomy. By incorporating this awareness on-line in the robot's controllers, AnDy paves the way for novel applications of physical human-robot collaboration in manufacturing, health-care, and assisted living.},
author = {Ivaldi, Serena and Fritzsche, Lars and Babi{\v{c}}, Jan and Stulp, Freek and Damsgaard, Michael and Graimann, Bernhard and Bellusci, Giovanni and Nori, Francesco and Ivaldi, Serena and Fritzsche, Lars and Babi{\v{c}}, Jan and Stulp, Freek and Damsgaard, Michael},
journal = {Digital Human Models (DHM)},
keywords = {MSD,anticipation,cobots,ergonomics,exoskeletons,human movement,humanoids,machine learning,prediction,robotics,wearable sensors},
title = {{Anticipatory models of human movements and dynamics : the roadmap of the AnDy project}},
year = {2017}
}
@inproceedings{Tortora2019,
abstract = {In this paper, we propose a novel human-robot interface capable to anticipate the user intention while performing reaching movements on a working bench in order to plan the action of a collaborative robot. The system integrates two levels of prediction: motion intention prediction, to detect movements onset and offset; motion direction prediction, based on Gaussian Mixture Model (GMM) trained with IMU and EMG data following an evidence accumulation approach. Novel dynamic stopping criteria have been proposed to flexibly adjust the trade-off between early anticipation and accuracy. Results show that our system outperforms previous methods, achieving a real-time classification accuracy of 94.3±2.9{\%} after 160.0msec±80.0msec from movement onset. The proposed interface can find many applications in the Industry 4.0 framework, where it is crucial for autonomous and collaborative robots to understand human movements as soon as possible to avoid accidents and injuries.},
archivePrefix = {arXiv},
arxivId = {1905.11734},
author = {Tortora, Stefano and Michieletto, Stefano and Stival, Francesca and Menegatti, Emanuele},
booktitle = {Proceedings of the IEEE 2019 9th International Conference on Cybernetics and Intelligent Systems and Robotics, Automation and Mechatronics, CIS and RAM 2019},
doi = {10.1109/CIS-RAM47153.2019.9095779},
eprint = {1905.11734},
isbn = {9781728134581},
keywords = {human-robot interface,movement prediction,multimodal classification},
title = {{Fast human motion prediction for human-robot collaboration with wearable interface}},
year = {2019}
}
@inproceedings{Gorur2018,
abstract = {We propose an architecture as a robots decision-making mechanism to anticipate a humans state of mind, and so plan accordingly during a human-robot collaboration task. At the core of the architecture lies a novel stochastic decision-making mechanism that implements a partially observable Markov decision process anticipating a humans state of mind in two-stages. In the first stage it anticipates the humans task related availability, intent (motivation), and capability during the collaboration. In the second, it further reasons about these states to anticipate the humans true need for help. Our contribution lies in the ability of our model to handle these unexpected conditions: 1) when the humans intention is estimated to be irrelevant to the assigned task and may be unknown to the robot, e.g., motivation is lost, another assignment is received, onset of tiredness, and 2) when the humans intention is relevant but the human doesnt want the robots assistance in the given context, e.g., because of the humans changing emotional states or the humans task-relevant distrust for the robot. Our results show that integrating this model into a robots decision-making process increases the efficiency and naturalness of the collaboration.},
author = {G{\"{o}}r{\"{u}}r, O. Can and Rosman, Benjamin and Sivrikaya, Fikret and Albayrak, Sahin},
booktitle = {ACM/IEEE International Conference on Human-Robot Interaction},
doi = {10.1145/3171221.3171256},
isbn = {9781450349536},
issn = {21672148},
keywords = {Anticipatory decision-making,Human-robot collaboration,Intent inference},
title = {{Social Cobots: Anticipatory Decision-Making for Collaborative Robots Incorporating Unexpected Human Behaviors}},
year = {2018}
}
@inproceedings{Jardim2017,
abstract = {{\textcopyright} 2017 SPIE. In our daily activities we perform prediction or anticipation when interacting with other humans or with objects. Prediction of human activity made by computers has several potential applications: Surveillance systems, human computer interfaces, sports video analysis, human-robot-collaboration, games and health-care. We propose a system capable of recognizing and predicting human actions using supervised classifiers trained with automatically labeled data evaluated in our human activity RGB-D dataset (recorded with a Kinect sensor) and using only the position of the main skeleton joints to extract features. Using conditional random fields (CRFs) to model the sequential nature of actions in a sequence has been used before, but where other approaches try to predict an outcome or anticipate ahead in time (seconds), we try to predict what will be the next action of a subject. Our results show an activity prediction accuracy of 89.9{\%} using an automatically labeled dataset.},
author = {Jardim, David and Nunes, Lu{\'{i}}s and Dias, Miguel},
booktitle = {Ninth International Conference on Machine Vision (ICMV 2016)},
doi = {10.1117/12.2268524},
isbn = {9781510611313},
issn = {1996756X},
title = {{Predicting human activities in sequences of actions in RGB-D videos}},
year = {2017}
}
@article{Riek2016,
abstract = {In order to be effective teammates, robots need to be able to understand high-level human behavior to recognize, anticipate, and adapt to human motion. We have designed a new approach to enable robots to perceive human group motion in real time to anticipate future actions and synthesize their own motion accordingly. We explore this within the context of joint action, in which humans and robots move together synchronously. In this paper we present an anticipation method, which takes high-level group behavior into account. We validate the method within a human–robot interaction scenario, inwhich an autonomous mobile robot observes a team of human dancers and then successfully and contingently coordinates its movements to “join the dance.” We compared the results of our anticipation method to move the robot with anothermethod that did not rely on high-level group behavior and found that ourmethod performed better both in terms ofmore closely synchronizing the robot'smotion to the team and exhibiting more contingent and fluent motion. These findings suggest that the robot performs better when it has an understanding of high- level group behavior than when it does not. This study will help enable others in the robotics community to build more fluent and adaptable robots in the future.},
author = {Riek, Laurel D and Member, Senior},
doi = {10.1109/TRO.2016.2570240},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
keywords = {Coordination,human–robot collaboration,human–robot joint action,movement analysis,synchronization},
title = {{Movement Coordination in Human – Robot Teams :}},
year = {2016}
}
@inproceedings{Alati2019,
abstract = {Robots assisting humans with some specific tasks have been demonstrated on several occasions. A further challenging idea is to anticipate human needs by mining the future demand from the next action prediction. To trigger this anticipation mechanism a robot has to recognize what the human is doing now, foresee what the human will do next, and from their connection guesstimating what to do to help. We propose here a deep network combining the essential components of this challenging process leading to foreseeing the help that can be provided in human-robot collaboration.},
author = {Alati, Edoardo and Mauro, Lorenzo and Ntouskos, Valsamis and Pirri, Fiora},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2019.8803155},
isbn = {9781538662496},
issn = {15224880},
keywords = {Deep learning,action recognition,activity recognition,human-robot collaboration,need for help recognition,scene segmentation},
title = {{Help by Predicting What to Do}},
year = {2019}
}
@article{Iqbal2016,
abstract = {In order to be effective teammates, robots need to be able to understand high-level human behavior to recognize, anticipate, and adapt to human motion. We have designed a new approach to enable robots to perceive human group motion in real time to anticipate future actions and synthesize their own motion accordingly. We explore this within the context of joint action, in which humans and robots move together synchronously. In this paper we present an anticipation method, which takes high-level group behavior into account. We validate the method within a human-robot interaction scenario, in which an autonomous mobile robot observes a team of human dancers and then successfully and contingently coordinates its movements to 'join the dance.' We compared the results of our anticipation method to move the robot with another method that did not rely on high-level group behavior and found that our method performed better both in terms of more closely synchronizing the robot's motion to the team and exhibiting more contingent and fluent motion. These findings suggest that the robot performs better when it has an understanding of high-level group behavior than when it does not. This study will help enable others in the robotics community to build more fluent and adaptable robots in the future.},
archivePrefix = {arXiv},
arxivId = {1605.01459},
author = {Iqbal, Tariq and Rack, Samantha and Riek, Laurel D.},
doi = {10.1109/TRO.2016.2570240},
eprint = {1605.01459},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Coordination,human-robot collaboration,human-robot joint action,movement analysis,synchronization},
title = {{Movement Coordination in Human-Robot Teams: A Dynamical Systems Approach}},
year = {2016}
}
@inproceedings{CanGorur2019,
abstract = {As a key component of collaborative robots (cobots) working with humans, existing decision-making approaches try to model the uncertainty in human behaviors as latent variables. However, as more possible contingencies are covered by such intention-aware models, they face slow convergence times and less accurate responses. For this purpose, we present a novel anticipatory policy selection mechanism built on existing intention-aware models, where a robot is required to choose from an existing set of policies based on an estimate of the human. Each of these intention-aware robot models anticipates and adapts to a different human's short-term changing behaviors. Our contribution is the Anticipatory Bayesian Policy Selection (ABPS) mechanism which selects from a library of different response policies that are generated from such models, and converges to a reliable policy after as few interactions as possible when faced with unknown humans. The selection is based on the estimation of the human in terms of long-term workplace characteristics that we call types, such as level of expertise, stamina, attention and collaborativeness. Our results show that incorporating this policy selection mechanism contributes positively to the efficiency and naturalness of the collaboration, when compared to the best intention-aware model in hindsight running alone.},
author = {{Can G{\"{o}}r{\"{u}}r}, O. and Rosman, Benjamin and Albayrak, Sahin},
booktitle = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
isbn = {9781510892002},
issn = {15582914},
keywords = {Anticipatory policy selection,Human type and intent inference,Human-Robot collaboration,Socially collaborative robots},
title = {{Anticipatory Bayesian policy selection for online adaptation of collaborative robots to unknown human types}},
year = {2019}
}
@article{Wang2021,
abstract = {Video action anticipation aims to predict future action categories from observed frames. Current state-of-the-art approaches mainly resort to recurrent neural networks to encode history information into hidden states, and predict future actions from the hidden representations. It is well known that the recurrent pipeline is inefficient in capturing long-term information which may limit its performance in predication task. To address this problem, this paper proposes a simple yet efficient Temporal Transformer with Progressive Prediction (TTPP) framework, which repurposes a Transformer-style architecture to aggregate observed features, and then leverages a light-weight network to progressively predict future features and actions. Specifically, predicted features along with predicted probabilities are accumulated into the inputs of subsequent prediction. We evaluate our approach on three action datasets, namely TVSeries, THUMOS-14, and TV-Human-Interaction. Additionally we also conduct a comprehensive study for several popular aggregation and prediction strategies. Extensive results show that TTPP not only outperforms the state-of-the-art methods but also more efficient.},
archivePrefix = {arXiv},
arxivId = {2003.03530},
author = {Wang, Wen and Peng, Xiaojiang and Su, Yanzhou and Qiao, Yu and Cheng, Jian},
doi = {10.1016/j.neucom.2021.01.087},
eprint = {2003.03530},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Action anticipation,Encoder-decoder,Progressive prediction,Transformer},
title = {{TTPP: Temporal Transformer with Progressive Prediction for efficient action anticipation}},
year = {2021}
}
@article{Sciutti2013,
abstract = { Understanding the goals of others is fundamental for any kind of interpersonal interaction and collaboration. From a neurocognitive perspective, intention understanding has been proposed to depend on an involvement of the observer's motor system in the prediction of the observed actions (Nystr{\"{o}}m et al. 2011; Rizzolatti {\&} Sinigaglia 2010; Southgate et al. 2009). An open question is if a similar understanding of the goal mediated by motor resonance can occur not only between humans, but also for humanoid robots. In this study we investigated whether goal-oriented robotic actions can induce motor resonance by measuring the appearance of anticipatory gaze shifts to the goal during action observation. Our results indicate a similar implicit processing of humans' and robots' actions and propose to use anticipatory gaze behaviour as a tool for the evaluation of human-robot interactions. Keywords: Humanoid robot; motor resonance; anticipation; proactive gaze; action understanding },
author = {Sciutti, Alessandra and Bisio, Ambra and Nori, Francesco and Metta, Giorgio and Fadiga, Luciano and Sandini, Giulio},
doi = {10.1075/is.14.3.02sci},
issn = {1572-0373},
journal = {Interaction Studies. Social Behaviour and Communication in Biological and Artificial Systems},
title = {{Robots can be perceived as goal-oriented agents}},
year = {2013}
}
@article{Dermy2017,
abstract = {This article describes our open-source software for predicting the intention of a user physically interacting with the humanoid robot iCub. Our goal is to allow the robot to infer the intention of the human partner during collaboration, by predicting the future intended trajectory: this capability is critical to design anticipatory behaviors that are crucial in human-robot collaborative scenarios, such as in co-manipulation, cooperative assembly, or transportation. We propose an approach to endow the iCub with basic capabilities of intention recognition, based on Probabilistic Movement Primitives (ProMPs), a versatile method for representing, generalizing, and reproducing complex motor skills. The robot learns a set of motion primitives from several demonstrations, provided by the human via physical interaction. During training, we model the collaborative scenario using human demonstrations. During the reproduction of the collaborative task, we use the acquired knowledge to recognize the intention of the human partner. Using a few early observations of the state of the robot, we can not only infer the intention of the partner but also complete the movement, even if the user breaks the physical interaction with the robot. We evaluate our approach in simulation and on the real iCub. In simulation, the iCub is driven by the user using the Geomagic Touch haptic device. In the real robot experiment, we directly interact with the iCub by grabbing and manually guiding the robot's arm. We realize two experiments on the real robot: one with simple reaching trajectories, and one inspired by collaborative object sorting. The software implementing our approach is open source and available on the GitHub platform. In addition, we provide tutorials and videos.},
author = {Dermy, Oriane and Paraschos, Alexandros and Ewerton, Marco and Peters, Jan and Charpillet, Fran{\c{c}}ois and Ivaldi, Serena},
doi = {10.3389/frobt.2017.00045},
issn = {22969144},
journal = {Frontiers Robotics AI},
keywords = {Intention,Interaction,Prediction,Probabilistic models,Robot},
title = {{Prediction of intention during interaction with iCub with probabilistic movement primitives}},
year = {2017}
}
@article{Canuto2021,
abstract = {To interact with humans in collaborative environments, machines need to be able to predict (i.e., anticipate) future events, and execute actions in a timely manner. However, the observation of the human limb movements may not be sufficient to anticipate their actions unambiguously. In this work, we consider two additional sources of information (i.e., context) over time, gaze, movement and object information, and study how these additional contextual cues improve the action anticipation performance. We address action anticipation as a classification task, where the model takes the available information as the input and predicts the most likely action. We propose to use the uncertainty about each prediction as an online decision-making criterion for action anticipation. Uncertainty is modeled as a stochastic process applied to a time-based neural network architecture, which improves the conventional class-likelihood (i.e., deterministic) criterion. The main contributions of this paper are fourfold: (i) We propose a novel and effective decision-making criterion that can be used to anticipate actions even in situations of high ambiguity; (ii) we propose a deep architecture that outperforms previous results in the action anticipation task when using the Acticipate collaborative dataset; (iii) we show that contextual information is important to disambiguate the interpretation of similar actions; and (iv) we also provide a formal description of three existing performance metrics that can be easily used to evaluate action anticipation models. Our results on the Acticipate dataset showed the importance of contextual information and the uncertainty criterion for action anticipation. We achieve an average accuracy of 98.75{\%} in the anticipation task using only an average of 25{\%} of observations. Also, considering that a good anticipation model should perform well in the action recognition task, we achieve an average accuracy of 100{\%} in action recognition on the Acticipate dataset, when the entire observation set is used.},
archivePrefix = {arXiv},
arxivId = {1910.00714},
author = {Canuto, Clebeson and Moreno, Plinio and Samatelo, Jorge and Vassallo, Raquel and Santos-Victor, Jos{\'{e}}},
doi = {10.1016/j.neucom.2020.07.135},
eprint = {1910.00714},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Action anticipation,Bayesian deep learning,Context information,Early action prediction,Uncertainty},
title = {{Action anticipation for collaborative environments: The impact of contextual information and uncertainty-based prediction}},
year = {2021}
}
@inproceedings{Huang2016,
abstract = {Efficient collaboration requires collaborators to monitor the behaviors of their partners, make inferences about their task intent, and plan their own actions accordingly. To work seamlessly and efficiently with their human counterparts, robots must similarly rely on predictions of their users' intent in planning their actions. In this paper, we present an anticipatory control method that enables robots to proactively perform task actions based on anticipated actions of their human partners. We implemented this method into a robot system that monitored its user's gaze, predicted his or her task intent based on observed gaze patterns, and performed anticipatory task actions according to its predictions. Results from a human-robot interaction experiment showed that anticipatory control enabled the robot to respond to user requests and complete the task faster-2.5 seconds on average and up to 3.4 seconds-compared to a robot using a reactive control method that did not anticipate user intent. Our findings highlight the promise of performing anticipatory actions for achieving efficient human-robot teamwork.},
author = {Huang, Chien Ming and Mutlu, Bilge},
booktitle = {ACM/IEEE International Conference on Human-Robot Interaction},
doi = {10.1109/HRI.2016.7451737},
isbn = {9781467383707},
issn = {21672148},
keywords = {Action observation,Anticipatory action,Gaze,Human-robot collaboration,Intent prediction},
title = {{Anticipatory robot control for efficient human-robot collaboration}},
year = {2016}
}
@article{Furnari2021,
abstract = {In this paper, we tackle the problem of egocentric action anticipation, i.e., predicting what actions the camera wearer will perform in the near future and which objects they will interact with. Specifically, we contribute Rolling-Unrolling LSTM, a learning architecture to anticipate actions from egocentric videos. The method is based on three components: 1) an architecture comprised of two LSTMs to model the sub-tasks of summarizing the past and inferring the future, 2) a Sequence Completion Pre-Training technique which encourages the LSTMs to focus on the different sub-tasks, and 3) a Modality ATTention (MATT) mechanism to efficiently fuse multi-modal predictions performed by processing RGB frames, optical flow fields and object-based features. The proposed approach is validated on EPIC-Kitchens, EGTEA Gaze+ and ActivityNet. The experiments show that the proposed architecture is state-of-the-art in the domain of egocentric videos, achieving top performances in the 2019 EPIC-Kitchens egocentric action anticipation challenge. The approach also achieves competitive performance on ActivityNet with respect to methods not based on unsupervised pre-training and generalizes to the tasks of early action recognition and action recognition. To encourage research on this challenging topic, we made our code, trained models, and pre-extracted features available at our web page: http://iplab.dmi.unict.it/rulstm.},
archivePrefix = {arXiv},
arxivId = {2005.02190},
author = {Furnari, Antonino and Farinella, Giovanni Maria},
doi = {10.1109/TPAMI.2020.2992889},
eprint = {2005.02190},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Action anticipation,LSTM,egocentric vision,recurrent neural networks},
pmid = {32386143},
title = {{Rolling-Unrolling LSTMs for Action Anticipation from First-Person Video}},
year = {2021}
}
@misc{An.Dy2017,
abstract = {Recent technological progress permits robots to actively and safely share a common workspace with humans. Europe currently leads the robotic market for safety-certified robots, by enabling robots to react to unintentional contacts. AnDy leverages these technologies and strengthens European leadership by endowing robots with the ability to control physical collaboration through intentional interaction.},
author = {An.Dy},
booktitle = {An.Dy},
title = {{AnDy - Advancing Anticipatory Behaviors in Dynamic Human-Robot Collaboration}},
year = {2017}
}
@misc{Smith2016,
abstract = {The purpose of this study was to provide a systematic review of action anticipation studies using functional neuroimaging or brain stimulation during a sport-specific anticipation task. A total of 15 studies from 2008 to 2014 were evaluated and are reported in four sections: expert-novice samples, action anticipation tasks, neuroimaging and stimulation techniques, and key findings. Investigators examined a wide range of action anticipation scenarios specific to eight different sports and utilized functional magnetic resonance imaging (fMRI), electroencephalogram (EEG), and transcranial magnetic stimulation (TMS). Expert-novice comparisons were commonly used to investigate differences in action anticipation performance and neurophysiology. Experts tended to outperform novices, and an extensive array of brain structures were reported to be involved differently for experts and novices during action anticipation. However, these neurophysiological findings were generally inconsistent across the studies reviewed. The discussion focuses on strengths and four key limitations. The conclusion posits remaining questions and recommendations for future research.},
author = {Smith, Daniel M.},
booktitle = {Neuroscience and Biobehavioral Reviews},
doi = {10.1016/j.neubiorev.2015.11.007},
issn = {18737528},
keywords = {Action anticipation,Athletes,Brain stimulation,FMRI,Neuroimaging},
pmid = {26616736},
title = {{Neurophysiology of action anticipation in athletes: A systematic review}},
year = {2016}
}
@inproceedings{Maeda2016,
abstract = {This paper introduces our initial investigation on the problem of providing a semi-autonomous robot collaborator with anticipative capabilities to predict human actions. Anticipative robot behavior is a desired characteristic of robot collaborators that lead to fluid, proactive interactions. We are particularly interested in improving reactive methods that rely on human action recognition to activate the corresponding robot action. Action recognition invariably causes delay in the robot's response, and the goal of our method is to eliminate this delay by predicting the next human action. Prediction is achieved by using a lookup table containing variations of assembly sequences, previously demonstrated by different users. The method uses the nearest neighbor sequence in the table that matches the actual sequence of human actions. At the movement level, our method uses a probabilistic representation of interaction primitives to generate robot trajectories. The method is demonstrated using a 7 degree-of-freedom lightweight arm equipped with a 5-finger hand on an assembly task consisting of 17 steps.},
author = {Maeda, Guilherme and Maloo, Aayush and Ewerton, Marco and Lioutikov, Rudolf and Peters, Jan},
booktitle = {AAAI Fall Symposium - Technical Report},
isbn = {9781577357759},
title = {{Anticipative interaction primitives for human-robot collaboration}},
year = {2016}
}
@article{Psarakis2022,
abstract = {The present study reports on a human-robot collaboration experiment involving an industrial task with the specific aim of exploring the effects of (i) fostering human anticipatory behavior towards the robot, through visual cues of the robot's next move and (ii) robot adaptiveness to the human actions through reducing its motion speed with respect to human movement's proximity. For investigating these effects a generic collaborative picking and sorting task was designed, implemented and tested by volunteer participants, in a Virtual Reality simulation environment. Results demonstrated that, showing robot's intent through anticipatory cues significantly increased team efficiency, human safety and collaborative fluency in conjunction with a positive subjective inclination towards the robot. Robot adaptiveness significantly increased human safety without decreasing task efficiency and fluency, compared to a control condition.},
author = {Psarakis, Loizos and Nathanael, Dimitris and Marmaras, Nicolas},
doi = {10.1016/j.ergon.2021.103241},
issn = {18728219},
journal = {International Journal of Industrial Ergonomics},
keywords = {Adaptive behavior,Anticipatory behavior,Human-robot collaboration,Visual cues},
title = {{Fostering short-term human anticipatory behavior in human-robot collaboration}},
year = {2022}
}
