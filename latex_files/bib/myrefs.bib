@article{Hellstrom2021,
   abstract = {<p>In this article, we investigate the role of causal reasoning in robotics research. Inspired by a categorization of human causal cognition, we propose a categorization of robot causal cognition. For each category, we identify related earlier work in robotics and also connect to research in other sciences. While the proposed categories mainly cover the sense–plan–act level of robotics, we also identify a number of higher-level aspects and areas of robotics research where causation plays an important role, for example, understandability, machine ethics, and robotics research methodology. Overall, we conclude that causation underlies several problem formulations in robotics, but it is still surprisingly absent in published research, in particular when it comes to explicit mentioning and using of causal concepts and terms. We discuss the reasons for, and consequences of, this and hope that this article clarifies the broad and deep connections between causal reasoning and robotics and also by pointing at the close connections to other research areas. At best, this will also contribute to a “causal revolution” in robotics.</p>},
   author = {Thomas Hellström},
   doi = {10.1515/pjbr-2021-0017},
   issn = {2081-4836},
   issue = {1},
   journal = {Paladyn, Journal of Behavioral Robotics},
   keywords = {causal cognition,causal inference,causal reasoning,causality,causation,counterfactual,intelligence,robotics},
   month = {4},
   pages = {238-255},
   publisher = {De Gruyter Open Ltd},
   title = {The relevance of causation in robotics: A review, categorization, and analysis},
   volume = {12},
   url = {https://www.degruyter.com/document/doi/10.1515/pjbr-2021-0017/html},
   year = {2021},
}
@article{Ahmed2020,
   abstract = {Despite recent successes of reinforcement learning (RL), it remains a challenge for agents to transfer learned skills to related environments. To facilitate research addressing this problem, we propose CausalWorld, a benchmark for causal structure and transfer learning in a robotic manipulation environment. The environment is a simulation of an open-source robotic platform, hence offering the possibility of sim-to-real transfer. Tasks consist of constructing 3D shapes from a given set of blocks - inspired by how children learn to build complex structures. The key strength of CausalWorld is that it provides a combinatorial family of such tasks with common causal structure and underlying factors (including, e.g., robot and object masses, colors, sizes). The user (or the agent) may intervene on all causal variables, which allows for fine-grained control over how similar different tasks (or task distributions) are. One can thus easily define training and evaluation distributions of a desired difficulty level, targeting a specific form of generalization (e.g., only changes in appearance or object mass). Further, this common parametrization facilitates defining curricula by interpolating between an initial and a target task. While users may define their own task distributions, we present eight meaningful distributions as concrete benchmarks, ranging from simple to very challenging, all of which require long-horizon planning as well as precise low-level motor control. Finally, we provide baseline results for a subset of these tasks on distinct training curricula and corresponding evaluation protocols, verifying the feasibility of the tasks in this benchmark.},
   author = {Ossama Ahmed and Frederik Träuble and Anirudh Goyal and Alexander Neitz and Yoshua Bengio and Bernhard Schölkopf and Manuel Wüthrich and Stefan Bauer},
   month = {10},
   title = {CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning},
   url = {http://arxiv.org/abs/2010.04296},
   year = {2020},
}
@book_section{Winfield2018,
   abstract = {The user has requested enhancement of the downloaded file.},
   author = {Alan F. T. Winfield and Verena V. Hafner},
   city = {Cham},
   doi = {10.1007/978-3-319-31737-3_73-1},
   journal = {Handbook of Anticipation},
   pages = {1-30},
   publisher = {Springer International Publishing},
   title = {Anticipation in Robotics},
   url = {http://link.springer.com/10.1007/978-3-319-31737-3_73-1},
   year = {2018},
}
@article{Semeraro2023,
   abstract = {Technological progress increasingly envisions the use of robots interacting with people in everyday life. Human–robot collaboration (HRC) is the approach that explores the interaction between a human and a robot, during the completion of a common objective, at the cognitive and physical level. In HRC works, a cognitive model is typically built, which collects inputs from the environment and from the user, elaborates and translates these into information that can be used by the robot itself. Machine learning is a recent approach to build the cognitive model and behavioural block, with high potential in HRC. Consequently, this paper proposes a thorough literature review of the use of machine learning techniques in the context of human–robot collaboration. 45 key papers were selected and analysed, and a clustering of works based on the type of collaborative tasks, evaluation metrics and cognitive variables modelled is proposed. Then, a deep analysis on different families of machine learning algorithms and their properties, along with the sensing modalities used, is carried out. Among the observations, it is outlined the importance of the machine learning algorithms to incorporate time dependencies. The salient features of these works are then cross-analysed to show trends in HRC and give guidelines for future works, comparing them with other aspects of HRC not appeared in the review.},
   author = {Francesco Semeraro and Alexander Griffiths and Angelo Cangelosi},
   doi = {10.1016/j.rcim.2022.102432},
   issn = {07365845},
   journal = {Robotics and Computer-Integrated Manufacturing},
   month = {2},
   pages = {102432},
   title = {Human–robot collaboration and machine learning: A systematic review of recent research},
   volume = {79},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0736584522001156},
   year = {2023},
}
@article{Zhang2022,
   abstract = {<p>Most robots are programmed to carry out specific tasks routinely with minor variations. However, more and more applications from SMEs require robots work alongside their counterpart human workers. To smooth the collaboration task flow and improve the collaboration efficiency, a better way is to formulate the robot to surmise what kind of assistance a human coworker needs and naturally take the right action at the right time. This paper proposes a prediction-based human-robot collaboration model for assembly scenarios. An embedded learning from demonstration technique enables the robot to understand various task descriptions and customized working preferences. A state-enhanced convolutional long short-term memory (ConvLSTM)-based framework is formulated for extracting the high-level spatiotemporal features from the shared workspace and predicting the future actions to facilitate the fluent task transition. This model allows the robot to adapt itself to predicted human actions and enables proactive assistance during collaboration. We applied our model to the seats assembly experiment for a scale model vehicle and it can obtain a human worker’s intentions, predict a coworker’s future actions, and provide assembly parts correspondingly. It has been verified that the proposed framework yields higher smoothness and shorter idle times, and meets more working styles, compared to the state-of-the-art methods without prediction awareness.</p>},
   author = {Zhujun Zhang and Gaoliang Peng and Weitian Wang and Yi Chen and Yunyi Jia and Shaohui Liu},
   doi = {10.3390/s22114279},
   issn = {1424-8220},
   issue = {11},
   journal = {Sensors},
   month = {6},
   pages = {4279},
   title = {Prediction-Based Human-Robot Collaboration in Assembly Tasks Using a Learning from Demonstration Model},
   volume = {22},
   url = {https://www.mdpi.com/1424-8220/22/11/4279},
   year = {2022},
}
@article{Mukherjee2022,
   abstract = {Increased global competition has placed a premium on customer satisfaction, and there is a greater demand for manufacturers to be flexible with their products and services. This challenge is usually addressed with the introduction of human operators for precise tasks that require dexterity, flexibility and cognitive decision making. On the other hand, robots, through automation, are very effective in carrying out repetitive, non-ergonomic tasks. Owing to the complementary nature of robots’ and humans’ capabilities, there is an increased interest towards a shared workspace for humans and robots to work together collaboratively, forming the motivation behind the field of human-robot collaboration (HRC). Research in HRC in industry is concerned with the safety of the humans and robots, extent, and modes of collaboration among them, and the level of autonomy and adaptability of robots that can be trained for different tasks. This paper introduces a novel taxonomy of levels of interaction between humans and robots along the lines of SAEs guidelines for autonomous vehicles in response to a need for standard definitions and evolving nature of the field. Research into modes of communication for HRC driven by machine learning are reviewed followed by broad definitions of the types of machine learning. The authors also present a comprehensive review of the machine learning (ML) methodologies and industrial applications of the same in the context of adaptable collaborative robots.},
   author = {Debasmita Mukherjee and Kashish Gupta and Li Hsin Chang and Homayoun Najjaran},
   doi = {10.1016/j.rcim.2021.102231},
   issn = {07365845},
   journal = {Robotics and Computer-Integrated Manufacturing},
   month = {2},
   pages = {102231},
   title = {A Survey of Robot Learning Strategies for Human-Robot Collaboration in Industrial Settings},
   volume = {73},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0736584521001137},
   year = {2022},
}
@inproceedings{Schydlo2018,
   abstract = {Close human-robot cooperation is a key enabler for new developments in advanced manufacturing and assistive applications. Close cooperation require robots that can predict human actions and intent, understanding human non-verbal cues. Recent approaches based on neural networks have led to encouraging results in the human action prediction problem both in continuous and discrete spaces. Our approach extends the research in this direction. Our contributions are three-fold. First, we validate the use of gaze and body pose cues as a means of predicting human action through a feature selection method. Next, we address two shortcomings of existing literature: predicting multiple and variable-length action sequences. This is achieved by applying an encoder-decoder recurrent neural network topology in the discrete action prediction problem. In addition, we theoretically demonstrate the importance of predicting multiple action sequences as a means of estimating the stochastic reward in a human robot cooperation scenario. Finally, we show the ability to effectively train the prediction model on an action prediction dataset, involving human motion data, and explore the influence of the model's parameters on its performance.},
   author = {Paul Schydlo and Mirko Rakovic and Lorenzo Jamone and Jose Santos-Victor},
   doi = {10.1109/ICRA.2018.8460924},
   isbn = {978-1-5386-3081-5},
   issn = {10504729},
   journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
   month = {5},
   pages = {1-6},
   publisher = {IEEE},
   title = {Anticipation in Human-Robot Cooperation: A Recurrent Neural Network Approach for Multiple Action Sequences Prediction},
   url = {https://ieeexplore.ieee.org/document/8460924/},
   year = {2018},
}
@article{Psarakis2022,
   abstract = {The present study reports on a human-robot collaboration experiment involving an industrial task with the specific aim of exploring the effects of (i) fostering human anticipatory behavior towards the robot, through visual cues of the robot's next move and (ii) robot adaptiveness to the human actions through reducing its motion speed with respect to human movement's proximity. For investigating these effects a generic collaborative picking and sorting task was designed, implemented and tested by volunteer participants, in a Virtual Reality simulation environment. Results demonstrated that, showing robot's intent through anticipatory cues significantly increased team efficiency, human safety and collaborative fluency in conjunction with a positive subjective inclination towards the robot. Robot adaptiveness significantly increased human safety without decreasing task efficiency and fluency, compared to a control condition.},
   author = {Loizos Psarakis and Dimitris Nathanael and Nicolas Marmaras},
   doi = {10.1016/j.ergon.2021.103241},
   issn = {01698141},
   journal = {International Journal of Industrial Ergonomics},
   keywords = {Adaptive behavior,Anticipatory behavior,Human-robot collaboration,Visual cues},
   month = {1},
   pages = {103241},
   title = {Fostering short-term human anticipatory behavior in human-robot collaboration},
   volume = {87},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0169814121001591},
   year = {2022},
}
@inproceedings{Maeda2016,
   abstract = {This paper introduces our initial investigation on the problem of providing a semi-autonomous robot collaborator with anticipative capabilities to predict human actions. Anticipative robot behavior is a desired characteristic of robot collaborators that lead to fluid, proactive interactions. We are particularly interested in improving reactive methods that rely on human action recognition to activate the corresponding robot action. Action recognition invariably causes delay in the robot's response, and the goal of our method is to eliminate this delay by predicting the next human action. Prediction is achieved by using a lookup table containing variations of assembly sequences, previously demonstrated by different users. The method uses the nearest neighbor sequence in the table that matches the actual sequence of human actions. At the movement level, our method uses a probabilistic representation of interaction primitives to generate robot trajectories. The method is demonstrated using a 7 degree-of-freedom lightweight arm equipped with a 5-finger hand on an assembly task consisting of 17 steps.},
   author = {Guilherme J. Maeda and Aayushi Maloo and Marco Ewerton and Rudolf Lioutikov and Jan Peters},
   isbn = {9781577357759},
   journal = {AAAI Fall Symposium - Technical Report},
   title = {Anticipative interaction primitives for human-robot collaboration},
   year = {2016},
}
@article{Furnari2021,
   abstract = {In this paper, we tackle the problem of egocentric action anticipation, i.e., predicting what actions the camera wearer will perform in the near future and which objects they will interact with. Specifically, we contribute Rolling-Unrolling LSTM, a learning architecture to anticipate actions from egocentric videos. The method is based on three components: 1) an architecture comprised of two LSTMs to model the sub-tasks of summarizing the past and inferring the future, 2) a Sequence Completion Pre-Training technique which encourages the LSTMs to focus on the different sub-tasks, and 3) a Modality ATTention (MATT) mechanism to efficiently fuse multi-modal predictions performed by processing RGB frames, optical flow fields and object-based features. The proposed approach is validated on EPIC-Kitchens, EGTEA Gaze+ and ActivityNet. The experiments show that the proposed architecture is state-of-the-art in the domain of egocentric videos, achieving top performances in the 2019 EPIC-Kitchens egocentric action anticipation challenge. The approach also achieves competitive performance on ActivityNet with respect to methods not based on unsupervised pre-training and generalizes to the tasks of early action recognition and action recognition. To encourage research on this challenging topic, we made our code, trained models, and pre-extracted features available at our web page: http://iplab.dmi.unict.it/rulstm.},
   author = {Antonino Furnari and Giovanni Maria Farinella},
   doi = {10.1109/TPAMI.2020.2992889},
   issn = {0162-8828},
   issue = {11},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   keywords = {Action anticipation,LSTM,egocentric vision,recurrent neural networks},
   month = {11},
   pages = {4021-4036},
   pmid = {32386143},
   title = {Rolling-Unrolling LSTMs for Action Anticipation from First-Person Video},
   volume = {43},
   url = {https://ieeexplore.ieee.org/document/9088213/},
   year = {2021},
}
@article{Canuto2021,
   abstract = {To interact with humans in collaborative environments, machines need to be able to predict (i.e., anticipate) future events, and execute actions in a timely manner. However, the observation of the human limb movements may not be sufficient to anticipate their actions unambiguously. In this work, we consider two additional sources of information (i.e., context) over time, gaze, movement and object information, and study how these additional contextual cues improve the action anticipation performance. We address action anticipation as a classification task, where the model takes the available information as the input and predicts the most likely action. We propose to use the uncertainty about each prediction as an online decision-making criterion for action anticipation. Uncertainty is modeled as a stochastic process applied to a time-based neural network architecture, which improves the conventional class-likelihood (i.e., deterministic) criterion. The main contributions of this paper are fourfold: (i) We propose a novel and effective decision-making criterion that can be used to anticipate actions even in situations of high ambiguity; (ii) we propose a deep architecture that outperforms previous results in the action anticipation task when using the Acticipate collaborative dataset; (iii) we show that contextual information is important to disambiguate the interpretation of similar actions; and (iv) we also provide a formal description of three existing performance metrics that can be easily used to evaluate action anticipation models. Our results on the Acticipate dataset showed the importance of contextual information and the uncertainty criterion for action anticipation. We achieve an average accuracy of 98.75\% in the anticipation task using only an average of 25\% of observations. Also, considering that a good anticipation model should perform well in the action recognition task, we achieve an average accuracy of 100\% in action recognition on the Acticipate dataset, when the entire observation set is used.},
   author = {Clebeson Canuto and Plinio Moreno and Jorge Samatelo and Raquel Vassallo and José Santos-Victor},
   doi = {10.1016/j.neucom.2020.07.135},
   issn = {09252312},
   journal = {Neurocomputing},
   keywords = {Action anticipation,Bayesian deep learning,Context information,Early action prediction,Uncertainty},
   month = {7},
   pages = {301-318},
   title = {Action anticipation for collaborative environments: The impact of contextual information and uncertainty-based prediction},
   volume = {444},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220317719},
   year = {2021},
}
@article{Wang2021,
   abstract = {Video action anticipation aims to predict future action categories from observed frames. Current state-of-the-art approaches mainly resort to recurrent neural networks to encode history information into hidden states, and predict future actions from the hidden representations. It is well known that the recurrent pipeline is inefficient in capturing long-term information which may limit its performance in predication task. To address this problem, this paper proposes a simple yet efficient Temporal Transformer with Progressive Prediction (TTPP) framework, which repurposes a Transformer-style architecture to aggregate observed features, and then leverages a light-weight network to progressively predict future features and actions. Specifically, predicted features along with predicted probabilities are accumulated into the inputs of subsequent prediction. We evaluate our approach on three action datasets, namely TVSeries, THUMOS-14, and TV-Human-Interaction. Additionally we also conduct a comprehensive study for several popular aggregation and prediction strategies. Extensive results show that TTPP not only outperforms the state-of-the-art methods but also more efficient.},
   author = {Wen Wang and Xiaojiang Peng and Yanzhou Su and Yu Qiao and Jian Cheng},
   doi = {10.1016/j.neucom.2021.01.087},
   issn = {09252312},
   journal = {Neurocomputing},
   keywords = {Action anticipation,Encoder-decoder,Progressive prediction,Transformer},
   month = {5},
   pages = {270-279},
   title = {TTPP: Temporal Transformer with Progressive Prediction for efficient action anticipation},
   volume = {438},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221001697},
   year = {2021},
}
@inproceedings{Gorur2018,
   abstract = {We propose an architecture as a robots decision-making mechanism to anticipate a humans state of mind, and so plan accordingly during a human-robot collaboration task. At the core of the architecture lies a novel stochastic decision-making mechanism that implements a partially observable Markov decision process anticipating a humans state of mind in two-stages. In the first stage it anticipates the humans task related availability, intent (motivation), and capability during the collaboration. In the second, it further reasons about these states to anticipate the humans true need for help. Our contribution lies in the ability of our model to handle these unexpected conditions: 1) when the humans intention is estimated to be irrelevant to the assigned task and may be unknown to the robot, e.g., motivation is lost, another assignment is received, onset of tiredness, and 2) when the humans intention is relevant but the human doesnt want the robots assistance in the given context, e.g., because of the humans changing emotional states or the humans task-relevant distrust for the robot. Our results show that integrating this model into a robots decision-making process increases the efficiency and naturalness of the collaboration.},
   author = {O. Can Görür and Benjamin Rosman and Fikret Sivrikaya and Sahin Albayrak},
   city = {New York, NY, USA},
   doi = {10.1145/3171221.3171256},
   isbn = {9781450349536},
   issn = {21672148},
   journal = {Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
   keywords = {Anticipatory decision-making,Human-robot collaboration,Intent inference},
   month = {2},
   pages = {398-406},
   publisher = {ACM},
   title = {Social Cobots},
   url = {https://dl.acm.org/doi/10.1145/3171221.3171256},
   year = {2018},
}
@book{Rodriguez2019,
   abstract = {Human action-anticipation methods predict what is the future action by observing only a few portion of an action in progress. This is critical for applications where computers have to react to human actions as early as possible such as autonomous driving, human-robotic interaction, assistive robotics among others. In this paper, we present a method for human action anticipation by predicting the most plausible future human motion. We represent human motion using Dynamic Images [1] and make use of tailored loss functions to encourage a generative model to produce accurate future motion prediction. Our method outperforms the currently best performing action-anticipation methods by 4\% on JHMDB-21, 5.2\% on UT-Interaction and 5.1\% on UCF 101-24 benchmarks.},
   author = {Cristian Rodriguez and Basura Fernando and Hongdong Li},
   doi = {10.1007/978-3-030-11015-4_10},
   isbn = {9783030110147},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Action-anticipation,Dynamic image,Generation,Motion representation,Prediction},
   pages = {89-105},
   title = {Action Anticipation by Predicting Future Dynamic Images},
   url = {http://link.springer.com/10.1007/978-3-030-11015-4_10},
   year = {2019},
}
@inproceedings{Gammulle2019,
   abstract = {Inspired by human neurological structures for action anticipation, we present an action anticipation model that enables the prediction of plausible future actions by forecasting both the visual and temporal future. In contrast to current state-of-the-art methods which first learn a model to predict future video features and then perform action anticipation using these features, the proposed framework jointly learns to perform the two tasks, future visual and temporal representation synthesis, and early action anticipation. The joint learning framework ensures that the predicted future embeddings are informative to the action anticipation task. Furthermore, through extensive experimental evaluations we demonstrate the utility of using both visual and temporal semantics of the scene, and illustrate how this representation synthesis could be achieved through a recurrent Generative Adversarial Network (GAN) framework. Our model outperforms the current state-of-the-art methods on multiple datasets: UCF101, UCF101-24, UT-Interaction and TV Human Interaction.},
   author = {Harshala Gammulle and Simon Denman and Sridha Sridharan and Clinton Fookes},
   doi = {10.1109/ICCV.2019.00566},
   isbn = {978-1-7281-4803-8},
   issn = {15505499},
   journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
   month = {10},
   pages = {5561-5570},
   publisher = {IEEE},
   title = {Predicting the Future: A Jointly Learnt Model for Action Anticipation},
   url = {https://ieeexplore.ieee.org/document/9009844/},
   year = {2019},
}
@article{Wu2021,
   abstract = {Anticipating actions before they are executed is crucial for a wide range of practical applications, including autonomous driving and robotics. In this paper, we study the egocentric action anticipation task, which predicts future action seconds before it is performed for egocentric videos. Previous approaches focus on summarizing the observed content and directly predicting future action based on past observations. We believe it would benefit the action anticipation if we could mine some cues to compensate for the missing information of the unobserved frames. We then propose to decompose the action anticipation into a series of future feature predictions. We imagine how the visual feature changes in the near future and then predicts future action labels based on these imagined representations. Differently, our ImagineRNN is optimized in a contrastive learning way instead of feature regression. We utilize a proxy task to train the ImagineRNN, i.e., selecting the correct future states from distractors. We further improve ImagineRNN by residual anticipation, i.e., changing its target to predicting the feature difference of adjacent frames instead of the frame content. This promotes the network to focus on our target, i.e., the future action, as the difference between adjacent frame features is more important for forecasting the future. Extensive experiments on two large-scale egocentric action datasets validate the effectiveness of our method. Our method significantly outperforms previous methods on both the seen test set and the unseen test set of the EPIC Kitchens Action Anticipation Challenge.},
   author = {Yu Wu and Linchao Zhu and Xiaohan Wang and Yi Yang and Fei Wu},
   doi = {10.1109/TIP.2020.3040521},
   issn = {1057-7149},
   journal = {IEEE Transactions on Image Processing},
   keywords = {Action anticipation,action prediction,egocentric videos},
   pages = {1143-1152},
   pmid = {33270562},
   title = {Learning to Anticipate Egocentric Actions by Imagination},
   volume = {30},
   url = {https://ieeexplore.ieee.org/document/9280353/},
   year = {2021},
}
@article{Moutinho2023,
   abstract = {Human–Robot Collaboration is a critical component of Industry 4.0, contributing to a transition towards more flexible production systems that are quickly adjustable to changing production requirements. This paper aims to increase the natural collaboration level of a robotic engine assembly station by proposing a cognitive system powered by computer vision and deep learning to interpret implicit communication cues of the operator. The proposed system, which is based on a residual convolutional neural network with 34 layers and a long-short term memory recurrent neural network (ResNet-34 + LSTM), obtains assembly context through action recognition of the tasks performed by the operator. The assembly context was then integrated in a collaborative assembly plan capable of autonomously commanding the robot tasks. The proposed model showed a great performance, achieving an accuracy of 96.65\% and a temporal mean intersection over union (mIoU) of 94.11\% for the action recognition of the considered assembly. Moreover, a task-oriented evaluation showed that the proposed cognitive system was able to leverage the performed human action recognition to command the adequate robot actions with near-perfect accuracy. As such, the proposed system was considered as successful at increasing the natural collaboration level of the considered assembly station.},
   author = {Duarte Moutinho and Luís F. Rocha and Carlos M. Costa and Luís F. Teixeira and Germano Veiga},
   doi = {10.1016/j.rcim.2022.102449},
   issn = {07365845},
   journal = {Robotics and Computer-Integrated Manufacturing},
   keywords = {Deep learning,Human action recognition,Human-robot collaboration},
   month = {4},
   pages = {102449},
   publisher = {Elsevier Ltd},
   title = {Deep learning-based human action recognition to leverage context awareness in collaborative assembly},
   volume = {80},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0736584522001314},
   year = {2023},
}
